{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/seanwoo/CSC311_final_project/cleaned_data/train_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"tasks_use_model\", \"suboptimal_example\"]:\n",
    "    df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "df[\"text\"] = df[\"tasks_use_model\"] + \" [SEP] \" + df[\"suboptimal_example\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "tokenized = df[\"text\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for toks in tokenized:\n",
    "    counter.update(toks)\n",
    "\n",
    "max_vocab = 10000  # or whatever\n",
    "most_common = counter.most_common(max_vocab - 2)\n",
    "\n",
    "word2id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (w, c) in enumerate(most_common, start=2):\n",
    "    word2id[w] = i\n",
    "\n",
    "V = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    return [word2id.get(t, 1) for t in tokens]  # 1 = <UNK>\n",
    "\n",
    "encoded = tokenized.apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 80  # pick based on length distribution\n",
    "\n",
    "def pad(seq, max_len=max_len):\n",
    "    seq = seq[:max_len]\n",
    "    return seq + [0] * (max_len - len(seq))  # 0 = <PAD>\n",
    "\n",
    "X_ids = np.array([pad(seq) for seq in encoded], dtype=np.int64)  # shape (N, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, y = np.unique(df[\"label\"], return_inverse=True)\n",
    "C = len(labels)      # number of classes\n",
    "N, T = X_ids.shape   # N samples, T tokens per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "D = 50   # embedding dimension\n",
    "H = 64   # hidden units\n",
    "\n",
    "E  = rng.normal(0, 0.1, size=(V, D))     # embeddings\n",
    "W1 = rng.normal(0, 0.1, size=(D, H))\n",
    "b1 = np.zeros(H)\n",
    "W2 = rng.normal(0, 0.1, size=(H, C))\n",
    "b2 = np.zeros(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X_ids):\n",
    "    # X_ids: (N, T)\n",
    "    emb = E[X_ids]                    # (N, T, D)\n",
    "    mask = (X_ids != 0)[..., None]    # (N, T, 1), 0 for PAD\n",
    "\n",
    "    # sum embeddings of non-pad tokens\n",
    "    sum_emb = (emb * mask).sum(axis=1)            # (N, D)\n",
    "    lengths = np.clip(mask.sum(axis=1), 1, None)  # (N, 1) avoid /0\n",
    "    pooled = sum_emb / lengths                    # (N, D)\n",
    "\n",
    "    # hidden layer\n",
    "    h_preact = pooled @ W1 + b1       # (N, H)\n",
    "    h = np.maximum(h_preact, 0.0)     # ReLU\n",
    "\n",
    "    # output logits\n",
    "    logits = h @ W2 + b2              # (N, C)\n",
    "    return pooled, h, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    exps = np.exp(logits)\n",
    "    return exps / exps.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, y):\n",
    "    # y: (N,) integer class labels\n",
    "    N = probs.shape[0]\n",
    "    return -np.log(probs[np.arange(N), y] + 1e-12).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_and_grads(X_ids, y):\n",
    "    global E, W1, b1, W2, b2\n",
    "\n",
    "    N, T = X_ids.shape\n",
    "    pooled, h, logits = forward(X_ids)\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    # loss\n",
    "    loss = cross_entropy(probs, y)\n",
    "\n",
    "    # one-hot labels\n",
    "    y_onehot = np.zeros_like(probs)\n",
    "    y_onehot[np.arange(N), y] = 1\n",
    "\n",
    "    # dL/dlogits\n",
    "    dlogits = (probs - y_onehot) / N    # (N, C)\n",
    "\n",
    "    # grads for W2, b2\n",
    "    dW2 = h.T @ dlogits                 # (H, C)\n",
    "    db2 = dlogits.sum(axis=0)           # (C,)\n",
    "\n",
    "    # back to hidden\n",
    "    dh = dlogits @ W2.T                 # (N, H)\n",
    "    dh_preact = dh * (h > 0)            # ReLU derivative\n",
    "\n",
    "    # grads W1, b1\n",
    "    dW1 = pooled.T @ dh_preact          # (D, H)\n",
    "    db1 = dh_preact.sum(axis=0)         # (H,)\n",
    "\n",
    "    # grad wrt pooled embeddings\n",
    "    dpooled = dh_preact @ W1.T          # (N, D)\n",
    "\n",
    "    # back through mean pooling to token embeddings\n",
    "    emb = E[X_ids]                      # (N, T, D)\n",
    "    mask = (X_ids != 0)[..., None]                                  # (N, T, 1)\n",
    "    lengths = np.clip((X_ids != 0).sum(axis=1, keepdims=True), 1, None)  # (N, 1)\n",
    "    lengths = lengths[..., None]                                    # (N, 1, 1)\n",
    "    demb = (dpooled[:, None, :] * mask) / lengths                   # (N, T, D)\n",
    "\n",
    "\n",
    "    # accumulate into dE (embedding matrix)\n",
    "    dE = np.zeros_like(E)\n",
    "    # vectorized \"scatter add\"\n",
    "    np.add.at(dE, X_ids.ravel(), demb.reshape(-1, D))\n",
    "    dE[0] = 0.0  # ignore PAD row\n",
    "\n",
    "    return loss, (dE, dW1, db1, dW2, db2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00, loss 1.0986\n",
      "epoch 05, loss 1.0986\n",
      "epoch 10, loss 1.0986\n",
      "epoch 15, loss 1.0986\n",
      "epoch 20, loss 1.0985\n",
      "epoch 25, loss 1.0985\n",
      "epoch 30, loss 1.0985\n",
      "epoch 35, loss 1.0985\n",
      "epoch 40, loss 1.0985\n",
      "epoch 45, loss 1.0985\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, (dE, dW1, db1, dW2, db2) = loss_and_grads(X_ids, y)\n",
    "\n",
    "    E  -= lr * dE\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch {epoch:02d}, loss {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

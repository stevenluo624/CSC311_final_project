{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 40) (576, 80)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"/Users/seanwoo/CSC311_final_project/cleaned_data/train_clean.csv\")\n",
    "\n",
    "for col in [\"tasks_use_model\", \"suboptimal_example\"]:\n",
    "    df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "def tokenize(s: str):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    return s.split()\n",
    "\n",
    "tokens_tasks = df[\"tasks_use_model\"].apply(tokenize)\n",
    "tokens_sub   = df[\"suboptimal_example\"].apply(tokenize)\n",
    "\n",
    "# ---- build separate vocabularies ----\n",
    "def build_vocab(token_series, max_vocab=10000):\n",
    "    counter = Counter()\n",
    "    for toks in token_series:\n",
    "        counter.update(toks)\n",
    "    most_common = counter.most_common(max_vocab - 2)\n",
    "    word2id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for i, (w, _) in enumerate(most_common, start=2):\n",
    "        word2id[w] = i\n",
    "    return word2id\n",
    "\n",
    "word2id_tasks = build_vocab(tokens_tasks, max_vocab=8000)\n",
    "word2id_sub   = build_vocab(tokens_sub,   max_vocab=8000)\n",
    "\n",
    "V_tasks = len(word2id_tasks)\n",
    "V_sub   = len(word2id_sub)\n",
    "\n",
    "def encode(tokens, word2id):\n",
    "    return [word2id.get(t, 1) for t in tokens]  # 1 = <UNK>\n",
    "\n",
    "encoded_tasks = tokens_tasks.apply(lambda ts: encode(ts, word2id_tasks))\n",
    "encoded_sub   = tokens_sub.apply(lambda ts: encode(ts, word2id_sub))\n",
    "\n",
    "# ---- pad to fixed lengths (can be different) ----\n",
    "max_len_tasks = 40\n",
    "max_len_sub   = 80\n",
    "\n",
    "def pad(seq, max_len):\n",
    "    seq = seq[:max_len]\n",
    "    return seq + [0] * (max_len - len(seq))   # 0 = <PAD>\n",
    "\n",
    "X_tasks_ids = np.array([pad(s, max_len_tasks) for s in encoded_tasks], dtype=np.int64)\n",
    "X_sub_ids   = np.array([pad(s, max_len_sub)   for s in encoded_sub],   dtype=np.int64)\n",
    "\n",
    "N = len(df)\n",
    "print(X_tasks_ids.shape, X_sub_ids.shape)  # (N, T1), (N, T2)\n",
    "\n",
    "labels, y = np.unique(df[\"label\"], return_inverse=True)\n",
    "C = len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "D = 50   # embedding dim for each field\n",
    "H = 64   # hidden units\n",
    "\n",
    "E_tasks = rng.normal(0, 0.1, size=(V_tasks, D))\n",
    "E_sub   = rng.normal(0, 0.1, size=(V_sub,   D))\n",
    "\n",
    "# pooled_tasks (D) + pooled_sub (D) -> concat (2D)\n",
    "W1 = rng.normal(0, 0.1, size=(2 * D, H))\n",
    "b1 = np.zeros(H)\n",
    "W2 = rng.normal(0, 0.1, size=(H, C))\n",
    "b2 = np.zeros(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X_tasks_ids, X_sub_ids):\n",
    "    # ----- tasks stream -----\n",
    "    emb_t = E_tasks[X_tasks_ids]               # (N, T1, D)\n",
    "    mask_t = (X_tasks_ids != 0)[..., None]     # (N, T1, 1)\n",
    "\n",
    "    sum_emb_t = (emb_t * mask_t).sum(axis=1)   # (N, D)\n",
    "    len_t = np.clip(mask_t.sum(axis=1), 1, None)  # (N, 1)\n",
    "    pooled_t = sum_emb_t / len_t               # (N, D)\n",
    "\n",
    "    # ----- suboptimal stream -----\n",
    "    emb_s = E_sub[X_sub_ids]                   # (N, T2, D)\n",
    "    mask_s = (X_sub_ids != 0)[..., None]       # (N, T2, 1)\n",
    "\n",
    "    sum_emb_s = (emb_s * mask_s).sum(axis=1)   # (N, D)\n",
    "    len_s = np.clip(mask_s.sum(axis=1), 1, None)  # (N, 1)\n",
    "    pooled_s = sum_emb_s / len_s               # (N, D)\n",
    "\n",
    "    # concat pooled representations\n",
    "    pooled = np.concatenate([pooled_t, pooled_s], axis=1)  # (N, 2D)\n",
    "\n",
    "    # hidden â†’ logits\n",
    "    h_preact = pooled @ W1 + b1               # (N, H)\n",
    "    h = np.maximum(h_preact, 0.0)             # ReLU\n",
    "    logits = h @ W2 + b2                      # (N, C)\n",
    "\n",
    "    return pooled_t, pooled_s, pooled, h, logits\n",
    "\n",
    "def softmax(logits):\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    exps = np.exp(logits)\n",
    "    return exps / exps.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, y):\n",
    "    N = probs.shape[0]\n",
    "    return -np.log(probs[np.arange(N), y] + 1e-12).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_and_grads(X_tasks_ids, X_sub_ids, y):\n",
    "    global E_tasks, E_sub, W1, b1, W2, b2\n",
    "\n",
    "    N = X_tasks_ids.shape[0]\n",
    "\n",
    "    pooled_t, pooled_s, pooled, h, logits = forward(X_tasks_ids, X_sub_ids)\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    loss = cross_entropy(probs, y)\n",
    "\n",
    "    # one-hot labels\n",
    "    y_onehot = np.zeros_like(probs)\n",
    "    y_onehot[np.arange(N), y] = 1\n",
    "\n",
    "    # dL/dlogits\n",
    "    dlogits = (probs - y_onehot) / N          # (N, C)\n",
    "\n",
    "    # W2, b2\n",
    "    dW2 = h.T @ dlogits                       # (H, C)\n",
    "    db2 = dlogits.sum(axis=0)                # (C,)\n",
    "\n",
    "    # back to hidden\n",
    "    dh = dlogits @ W2.T                      # (N, H)\n",
    "    dh_preact = dh * (h > 0)                 # ReLU\n",
    "\n",
    "    # W1, b1\n",
    "    dW1 = pooled.T @ dh_preact               # (2D, H)\n",
    "    db1 = dh_preact.sum(axis=0)              # (H,)\n",
    "\n",
    "    # gradient wrt concatenated pooled representation\n",
    "    dpooled = dh_preact @ W1.T               # (N, 2D)\n",
    "    dpooled_t = dpooled[:, :D]               # (N, D)\n",
    "    dpooled_s = dpooled[:, D:]               # (N, D)\n",
    "\n",
    "    # ----- back to token embeddings: tasks -----\n",
    "    mask_t = (X_tasks_ids != 0)[..., None]   # (N, T1, 1)\n",
    "    len_t = np.clip((X_tasks_ids != 0).sum(axis=1, keepdims=True), 1, None)\n",
    "    len_t = len_t[..., None]                 # (N, 1, 1)\n",
    "\n",
    "    demb_t = (dpooled_t[:, None, :] * mask_t) / len_t  # (N, T1, D)\n",
    "\n",
    "    dE_tasks = np.zeros_like(E_tasks)\n",
    "    np.add.at(dE_tasks, X_tasks_ids.ravel(), demb_t.reshape(-1, D))\n",
    "    dE_tasks[0] = 0.0   # don't update PAD\n",
    "\n",
    "    # ----- back to token embeddings: suboptimal -----\n",
    "    mask_s = (X_sub_ids != 0)[..., None]     # (N, T2, 1)\n",
    "    len_s = np.clip((X_sub_ids != 0).sum(axis=1, keepdims=True), 1, None)\n",
    "    len_s = len_s[..., None]                 # (N, 1, 1)\n",
    "\n",
    "    demb_s = (dpooled_s[:, None, :] * mask_s) / len_s  # (N, T2, D)\n",
    "\n",
    "    dE_sub = np.zeros_like(E_sub)\n",
    "    np.add.at(dE_sub, X_sub_ids.ravel(), demb_s.reshape(-1, D))\n",
    "    dE_sub[0] = 0.0   # don't update PAD\n",
    "\n",
    "    return loss, (dE_tasks, dE_sub, dW1, db1, dW2, db2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X_tasks, X_sub, y_true):\n",
    "    _, _, _, _, logits = forward(X_tasks, X_sub)\n",
    "    probs = softmax(logits)\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    return (y_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 | train loss 1.0998, train acc 0.333 | \n",
      "epoch 05 | train loss 1.0998, train acc 0.333 | \n",
      "epoch 10 | train loss 1.0997, train acc 0.337 | \n",
      "epoch 15 | train loss 1.0997, train acc 0.337 | \n",
      "epoch 20 | train loss 1.0997, train acc 0.339 | \n",
      "epoch 25 | train loss 1.0996, train acc 0.344 | \n",
      "epoch 30 | train loss 1.0996, train acc 0.344 | \n",
      "epoch 35 | train loss 1.0995, train acc 0.344 | \n",
      "epoch 40 | train loss 1.0995, train acc 0.345 | \n",
      "epoch 45 | train loss 1.0995, train acc 0.340 | \n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train step on TRAIN SET\n",
    "    loss_train, (dE_tasks, dE_sub, dW1, db1, dW2, db2) = loss_and_grads(\n",
    "        X_tasks_ids, X_sub_ids, y\n",
    "    )\n",
    "\n",
    "    # parameter updates\n",
    "    E_tasks -= lr * dE_tasks\n",
    "    E_sub   -= lr * dE_sub\n",
    "    W1      -= lr * dW1\n",
    "    b1      -= lr * db1\n",
    "    W2      -= lr * dW2\n",
    "    b2      -= lr * db2\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        train_acc = accuracy(X_tasks_ids, X_sub_ids, y)\n",
    "\n",
    "        print(\n",
    "            f\"epoch {epoch:02d} | \"\n",
    "            f\"train loss {loss_train:.4f}, train acc {train_acc:.3f} | \"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 1 0 1 1 2 2 2 1 1 1 1 0 1 1 1 1 2 0 1 0 0 1 0 1 0 2 2 1 1 0 0 2 2 2\n",
      " 0 1 1 2 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 2 1 0\n",
      " 0 0 0 0 1 0 2 2 1 2 2 1 2 1 0 0 0 0 2 1 1 0 1 1 2 2 1 0 0 1 0 1 1 0 1 0 2\n",
      " 1 0 1 0 0 1 0 1 2 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 2 1 1 1 0 0 0 1 0 0 1 1 0 2 0 1 1 2 0 2\n",
      " 0 1 0 2 1 1 0 1 1 1 1 0 2 0 0 1 1 1 1 1 2 1 1 2 0 1 1 0 2 0 0 0 1 1 0 1 0\n",
      " 0 2 0 1 0 1 1 1 0 1 1 0 0 0 1 1 2 1 1 2 0 0 0 0 0 0 1 2 0 0 0 2 2 1 0 0 0\n",
      " 1 0 1 2 2 0 2 0 1 1 0 1 2 2 1 1 1 1 2 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
      " 1 0 0 1 0 1 0 1 2 2 0 1 1 1 0 0 0 1 2 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0\n",
      " 0 1 1 0 1 1 1 0 0 1 1 2 0 1 0 2 0 0 0 1 2 1 0 0 0 1 0 0 1 2 0 1 1 1 1 1 0\n",
      " 0 0 1 0 1 1 0 0 1 0 1 1 1 2 0 1 1 1 2 0 0 0 1 0 0 1 0 2 1 0 1 1 0 0 1 1 2\n",
      " 2 0 0 1 0 0 0 1 0 2 0 1 0 0 0 0 1 1 1 2 0 2 0 1 0 1 0 1 2 2 1 0 0 1 1 1 2\n",
      " 1 0 1 1 1 0 1 0 1 1 2 1 1 1 0 1 1 0 0 1 0 1 0 1 1 2 1 0 1 0 0 1 0 0 1 1 2\n",
      " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 2 1 2 0\n",
      " 1 0 0 0 2 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 2 1 0 2 2 0 0 2 0 0 0 1 2 1 0 2 0\n",
      " 0 1 1 0 0 0 0 1 0 0 2 0 1 1 1 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, logits_test = forward(X_tasks_ids, X_sub_ids)\n",
    "probs_test = softmax(logits_test)\n",
    "y_pred = probs_test.argmax(axis=1)\n",
    "predicted_labels = labels[y_pred]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

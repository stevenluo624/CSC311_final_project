{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8de231",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8309fc",
   "metadata": {},
   "source": [
    "## 1. Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8105b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"../cleaned_data/train_clean.csv\")\n",
    "valid = pd.read_csv(\"../cleaned_data/validation_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf9924",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing\n",
    "### 2.1 Combine and clean text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b931f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text columns into a single column because Naive Bayes works on a single text input\n",
    "text_cols = [\"tasks_use_model\", \"suboptimal_example\", \"verify_method\"]\n",
    "\n",
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train = combine_text(train)\n",
    "valid = combine_text(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c8a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing: lowercase, remove punctuation, extra spaces\n",
    "import re\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "train[\"full_text\"] = train[\"full_text\"].apply(clean_text)\n",
    "valid[\"full_text\"] = valid[\"full_text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768482d0",
   "metadata": {},
   "source": [
    "### 2.2 Select numeric and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0de4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric features\n",
    "ordinal_cols = [\n",
    "    \"academic_use_likelihood\",\n",
    "    \"suboptimal_frequency\",\n",
    "    \"reference_expectation\",\n",
    "    \"verify_frequency\"\n",
    "]\n",
    "\n",
    "# List of categorical features\n",
    "categorical_cols = [c for c in train.columns\n",
    "                    if c.startswith(\"best_task_types_\") \n",
    "                    or c.startswith(\"suboptimal_task_types_\")]\n",
    "\n",
    "# Combine all feature columns\n",
    "feature_cols = ordinal_cols + categorical_cols\n",
    "\n",
    "# Prepare numeric feature matrices\n",
    "X_train_numeric = train[feature_cols].values\n",
    "X_valid_numeric = valid[feature_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4f9b7",
   "metadata": {},
   "source": [
    "## 3. Vectorize text and build feature matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840c255f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 3000), (123, 3000))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize text data using Bag-of-Words\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_train_text = vectorizer.fit_transform(train[\"full_text\"])         # Fit on training data -> create vocabulary\n",
    "X_valid_text = vectorizer.transform(valid[\"full_text\"])             # Transform validation data -> use same vocabulary\n",
    "\n",
    "X_train_text.shape, X_valid_text.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc7b665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine text and numeric features\n",
    "X_train = hstack([X_train_text, X_train_numeric])\n",
    "X_valid = hstack([X_valid_text, X_valid_numeric])\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train[\"label\"].values\n",
    "y_valid = valid[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674f06c",
   "metadata": {},
   "source": [
    "## 4. Train Naive Bayes baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d2e9a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (alpha=1.0): 0.890625\n",
      "Validation accuracy (alpha=1.0): 0.6097560975609756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.696     0.780     0.736        41\n",
      "           1      0.556     0.610     0.581        41\n",
      "           2      0.562     0.439     0.493        41\n",
      "\n",
      "    accuracy                          0.610       123\n",
      "   macro avg      0.605     0.610     0.603       123\n",
      "weighted avg      0.605     0.610     0.603       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Simple baseline with alpha = 1.0 -> Laplace smoothing factor\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = nb.predict(X_train)\n",
    "y_valid_pred = nb.predict(X_valid)\n",
    "\n",
    "print(\"Training accuracy (alpha=1.0):\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Validation accuracy (alpha=1.0):\", accuracy_score(y_valid, y_valid_pred))\n",
    "print(classification_report(y_valid, y_valid_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c865f3b",
   "metadata": {},
   "source": [
    "## 5. With vs without stopwords\n",
    "### 5.1 Compare Bag-of-Words with vs without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6e5fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- stop_words = False ---\n",
      "Validation accuracy: 0.6097560975609756\n",
      "\n",
      "--- stop_words = True ---\n",
      "Validation accuracy: 0.6016260162601627\n",
      "\n",
      "Best setting: stop_words = False with accuracy 0.6097560975609756\n"
     ]
    }
   ],
   "source": [
    "best_use_stopwords = None\n",
    "best_acc = -np.inf\n",
    "\n",
    "for use_stopwords in [False, True]:\n",
    "    print(\"\\n--- stop_words =\", use_stopwords, \"---\")\n",
    "    \n",
    "    if use_stopwords:\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=3000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            stop_words='english'\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=3000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2\n",
    "        )\n",
    "\n",
    "    # Recompute text features for this setting\n",
    "    X_train_text = vectorizer.fit_transform(train[\"full_text\"])\n",
    "    X_valid_text = vectorizer.transform(valid[\"full_text\"])\n",
    "\n",
    "    # Combine with numeric features\n",
    "    X_train_tmp = hstack([X_train_text, X_train_numeric])\n",
    "    X_valid_tmp = hstack([X_valid_text, X_valid_numeric])\n",
    "\n",
    "    # Train a quick NB with fixed alpha\n",
    "    nb = MultinomialNB(alpha=1.0)\n",
    "    nb.fit(X_train_tmp, y_train)\n",
    "    y_pred = nb.predict(X_valid_tmp)\n",
    "\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    print(\"Validation accuracy:\", acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_use_stopwords = use_stopwords\n",
    "\n",
    "print(\"\\nBest setting: stop_words =\", best_use_stopwords, \"with accuracy\", best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc772693",
   "metadata": {},
   "source": [
    "### 5.2 Rebuild features using the best stopword setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b831706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best stopword configuration for the rest of the notebook\n",
    "if best_use_stopwords:\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=3000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "else:\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=3000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2\n",
    "    )\n",
    "\n",
    "# Recompute text features with the best setting\n",
    "X_train_text = vectorizer.fit_transform(train[\"full_text\"])\n",
    "X_valid_text = vectorizer.transform(valid[\"full_text\"])\n",
    "\n",
    "# Combine with numeric features\n",
    "from scipy.sparse import hstack\n",
    "X_train = hstack([X_train_text, X_train_numeric])\n",
    "X_valid = hstack([X_valid_text, X_valid_numeric])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a04bdc",
   "metadata": {},
   "source": [
    "## 6. 5-fold GroupKFold cross-validation to tune alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4420f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.1, mean CV accuracy = 0.6026\n",
      "alpha = 0.5, mean CV accuracy = 0.6306\n",
      "alpha = 1.0, mean CV accuracy = 0.6426\n",
      "alpha = 2.0, mean CV accuracy = 0.6268\n",
      "Best alpha from CV: 1.0 with accuracy 0.6426000899685109\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# csr_matrix format for efficient row slicing\n",
    "X_train = csr_matrix(X_train)\n",
    "\n",
    "alphas = [0.1, 0.5, 1.0, 2.0]\n",
    "groups = train[\"student_id\"].values   # group by student\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "cv_results = {}\n",
    "\n",
    "# Cross-validation to find best alpha\n",
    "for a in alphas:\n",
    "    fold_accuracies = []\n",
    "\n",
    "    # Split data into training and validation folds\n",
    "    for train_idx, val_idx in gkf.split(X_train, y_train, groups=groups):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # Train Naive Bayes model\n",
    "        nb = MultinomialNB(alpha=a)\n",
    "        nb.fit(X_tr, y_tr)\n",
    "        y_val_pred = nb.predict(X_val)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        fold_accuracies.append(acc)\n",
    "\n",
    "    # Compute mean accuracy across folds\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    cv_results[a] = mean_acc\n",
    "    print(f\"alpha = {a}, mean CV accuracy = {mean_acc:.4f}\")\n",
    "\n",
    "# Select best alpha\n",
    "best_alpha = max(cv_results, key=cv_results.get)\n",
    "print(\"Best alpha from CV:\", best_alpha, \"with accuracy\", cv_results[best_alpha])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e2b63",
   "metadata": {},
   "source": [
    "## 7. Final model trained on full training set and evaluated on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff459e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8472222222222222\n",
      "Final validation accuracy: 0.6097560975609756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.696     0.780     0.736        41\n",
      "           1      0.556     0.610     0.581        41\n",
      "           2      0.562     0.439     0.493        41\n",
      "\n",
      "    accuracy                          0.610       123\n",
      "   macro avg      0.605     0.610     0.603       123\n",
      "weighted avg      0.605     0.610     0.603       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_nb = MultinomialNB(alpha=best_alpha)\n",
    "final_nb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = nb.predict(X_train)\n",
    "y_valid_pred = final_nb.predict(X_valid)\n",
    "\n",
    "print(\"Training accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Final validation accuracy:\", accuracy_score(y_valid, y_valid_pred))\n",
    "print(classification_report(y_valid, y_valid_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

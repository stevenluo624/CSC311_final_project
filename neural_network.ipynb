{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0992197",
   "metadata": {},
   "source": [
    "# Clean Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538377",
   "metadata": {},
   "source": [
    "## Import and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98909075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Neural Network Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 80\n",
    "VOCAB_SIZE = 500  # Prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e177a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = 'cleaned_data'\n",
    "train_file = 'train_clean.csv'\n",
    "valid_file = 'validation_clean.csv'\n",
    "test_file = 'test_clean.csv'\n",
    "path_to_train = os.path.join(curr_dir, data_dir, train_file)\n",
    "path_to_valid = os.path.join(curr_dir, data_dir, valid_file)\n",
    "path_to_test = os.path.join(curr_dir, data_dir, test_file)\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "valid_df = pd.read_csv(path_to_valid)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "valid_df = pd.DataFrame(valid_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1c950",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# suboptimal_example is too noisy will thus be dropped\n",
    "text_cols = [\"tasks_use_model\", \"verify_method\"]\n",
    "numeric_cols = [\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                \"reference_expectation\", \"verify_frequency\"]\n",
    "binary_cols = [c for c in train_df.columns if \"task_types\" in c]\n",
    "\n",
    "best_task_cols = [c for c in train_df.columns if \"best_task_types\" in c]\n",
    "suboptimal_task_cols = [c for c in train_df.columns if \"suboptimal_task_types\" in c]\n",
    "\n",
    "def add_task_sum(df):\n",
    "    df['best_task_count'] = df[best_task_cols].sum(axis=1)\n",
    "    df['suboptimal_task_count'] = df[suboptimal_task_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = add_task_sum(train_df)\n",
    "valid_df = add_task_sum(valid_df)\n",
    "test_df = add_task_sum(test_df)\n",
    "\n",
    "def clean_text(s):\n",
    "    # Convert input to string, handling NaNs, floats, etc.\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s)  \n",
    "    \n",
    "    # Your original cleaning logic\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in text_cols:\n",
    "    # Use .astype(str) on the column before applying for extra safety, \n",
    "    # or just use the improved clean_text function.\n",
    "    train_df[col] = train_df[col].apply(clean_text) \n",
    "    valid_df[col] = valid_df[col].apply(clean_text)\n",
    "    test_df[col] = test_df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df47c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = combine_text(train_df)\n",
    "valid_df = combine_text(valid_df)\n",
    "test_df = combine_text(test_df)\n",
    "\n",
    "# combined_text = train_df[\"full_text\"] + test_df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary (Top 300 words only)\n",
    "word_counts = Counter(train_df[\"full_text\"].str.cat(sep=\" \").split())\n",
    "vocab_list = sorted([w for w, c in word_counts.most_common(VOCAB_SIZE)])\n",
    "vocab_map = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe6743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(df, text_series, vocab_map, is_train=True):\n",
    "    \n",
    "    # Extract original numeric cols\n",
    "    orig_nums = df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                    \"reference_expectation\", \"verify_frequency\"]].values\n",
    "    scaled_orig = (orig_nums - 3.0) / 1.2\n",
    "    \n",
    "    # Scale the new feature separately\n",
    "    task_count = df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "    scaled_count = (task_count - 2.0) / 1.5 \n",
    "\n",
    "    X_num = np.hstack([scaled_orig, scaled_count])\n",
    "    \n",
    "    # 2. Binary Features\n",
    "    X_bin = df[binary_cols].values\n",
    "    \n",
    "    # 3. Bag of Words (Log Scaled)\n",
    "    X_bow = np.zeros((len(df), len(vocab_map)), dtype=np.float32)\n",
    "    for i, text in enumerate(text_series):\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            if w in vocab_map:\n",
    "                X_bow[i, vocab_map[w]] += 1\n",
    "    X_bow = np.log1p(X_bow)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.hstack([X_num, X_bin, X_bow]).astype(np.float32)\n",
    "    \n",
    "    if is_train or 'label' in df.columns:\n",
    "        y = df['label'].values.astype(np.int64)\n",
    "        return X, y\n",
    "    return X, None\n",
    "\n",
    "X_train, y_train = get_features_labels(train_df, train_df[\"full_text\"], vocab_map)\n",
    "X_valid, y_valid = get_features_labels(valid_df, valid_df[\"full_text\"], vocab_map)\n",
    "X_test, y_test = get_features_labels(test_df, test_df[\"full_text\"], vocab_map)\n",
    "\n",
    "# Loaders\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_tensor = torch.utils.data.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "valid_loader = torch.utils.data.DataLoader(valid_tensor, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c5b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4) # High dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48484b4c",
   "metadata": {},
   "source": [
    "## Grid Search for best learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2372cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_search(lr):\n",
    "#     # Re-initialize model for each run to start fresh\n",
    "#     model = MLP(input_dim=X_train.shape[1]) \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     # Using AdamW is generally better for stabilization\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    \n",
    "#     # Scheduler (optional for search, but good to keep if used in final model)\n",
    "#     # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "#     best_acc = 0.0\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         # --- TRAIN PHASE ---\n",
    "#         model.train()\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_batch)\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         # --- VALIDATION PHASE ---\n",
    "#         model.eval()\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X_valid, y_valid in valid_loader:\n",
    "#                 outputs = model(X_valid)\n",
    "#                 preds = torch.argmax(outputs, dim=1)\n",
    "#                 val_correct += (preds == y_valid).sum().item()\n",
    "#                 val_total += y_valid.size(0)\n",
    "                \n",
    "#         val_acc = val_correct / val_total\n",
    "\n",
    "#         # Checkpoint based on Validation Accuracy\n",
    "#         if val_acc > best_acc:\n",
    "#             best_acc = val_acc\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#     # Load best weights to evaluate on Test Set\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Ensure X_test is a tensor\n",
    "#         test_tensor = torch.tensor(X_test) if not isinstance(X_test, torch.Tensor) else X_test\n",
    "#         logits = model(test_tensor)\n",
    "#         preds = torch.argmax(logits, dim=1)\n",
    "#         # Calculate final test accuracy\n",
    "#         final_acc = (preds == torch.tensor(y_test)).float().mean().item()\n",
    "    \n",
    "#     return final_acc, best_model_wts\n",
    "\n",
    "# # --- GRID SEARCH LOOP ---\n",
    "# learning_rates = [0.005, 0.002, 0.001, 0.0005, 0.0001]\n",
    "# best_overall_acc = 0.0\n",
    "# best_lr = 0.0\n",
    "\n",
    "# print(f\"{'Learning Rate':<15} | {'Test Accuracy':<15}\")\n",
    "# print(\"-\" * 35)\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     acc, wts = train_model_search(lr)\n",
    "#     print(f\"{lr:<15} | {acc:.4f}\")\n",
    "    \n",
    "#     if acc > best_overall_acc:\n",
    "#         best_overall_acc = acc\n",
    "#         best_lr = lr\n",
    "#         best_wts = wts\n",
    "\n",
    "# print(\"-\" * 35)\n",
    "# print(f\"Best Accuracy: {best_overall_acc:.4f} with Learning Rate: {best_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f874",
   "metadata": {},
   "source": [
    "## Find Best Validation Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  | Train Acc  | Val Acc   \n",
      "------------------------------\n",
      "1      | 0.3420     | 0.3740     *\n",
      "2      | 0.4792     | 0.5285     *\n",
      "3      | 0.5312     | 0.5366     *\n",
      "4      | 0.5764     | 0.6016     *\n",
      "5      | 0.6510     | 0.6748     *\n",
      "6      | 0.6858     | 0.7154     *\n",
      "9      | 0.7847     | 0.7236     *\n",
      "10     | 0.8090     | 0.6829    \n",
      "20     | 0.9132     | 0.6992    \n",
      "30     | 0.9653     | 0.6585    \n",
      "40     | 0.9809     | 0.6504    \n",
      "50     | 0.9861     | 0.6341    \n",
      "60     | 0.9861     | 0.6423    \n",
      "70     | 0.9948     | 0.6423    \n",
      "80     | 0.9913     | 0.6423    \n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(f\"{'Epoch':<6} | {'Train Acc':<10} | {'Val Acc':<10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    for X_b, y_b in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_b)\n",
    "        loss = criterion(outputs, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_b).sum().item()\n",
    "        total += y_b.size(0)\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    v_correct, v_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_v, y_v in valid_loader:\n",
    "            outputs = model(X_v)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            v_correct += (preds == y_v).sum().item()\n",
    "            v_total += y_v.size(0)\n",
    "    val_acc = v_correct / v_total\n",
    "    \n",
    "    # Checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(f\"{epoch+1:<6} | {train_acc:<10.4f} | {val_acc:<10.4f} *\")\n",
    "    elif (epoch+1) % 10 == 0:\n",
    "        print(f\"{epoch+1:<6} | {train_acc:<10.4f} | {val_acc:<10.4f}\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08e4ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7142857313156128\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Ensure X_test is a tensor\n",
    "    test_tensor = torch.tensor(X_test) if not isinstance(X_test, torch.Tensor) else X_test\n",
    "    logits = model(test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    # Calculate final test accuracy\n",
    "    final_acc = (preds == torch.tensor(y_test)).float().mean().item()\n",
    "    print(f\"Test Accuracy: {final_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655d72a",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbf66543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 'model_artifacts.json'.\n"
     ]
    }
   ],
   "source": [
    "# model = MLP(input_dim=X_train.shape[1])\n",
    "# model.load_state_dict(best_wts)\n",
    "\n",
    "# Extract weights (Transposed for x @ W + b shape)\n",
    "# We convert numpy arrays to lists so they can be serialized to JSON\n",
    "artifacts = {\n",
    "    \"W1\": model.fc1.weight.detach().numpy().T.tolist(),\n",
    "    \"b1\": model.fc1.bias.detach().numpy().tolist(),\n",
    "    \"W2\": model.fc2.weight.detach().numpy().T.tolist(),\n",
    "    \"b2\": model.fc2.bias.detach().numpy().tolist(),\n",
    "    \"W3\": model.fc3.weight.detach().numpy().T.tolist(),\n",
    "    \"b3\": model.fc3.bias.detach().numpy().tolist(),\n",
    "    \"vocab_list\": vocab_list,\n",
    "    \"binary_cols\": binary_cols,\n",
    "    \"best_task_cols\": best_task_cols,\n",
    "    \"suboptimal_task_cols\": suboptimal_task_cols\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open('model_artifacts.json', 'w') as f:\n",
    "    json.dump(artifacts, f)\n",
    "\n",
    "print(\"Successfully saved 'model_artifacts.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0992197",
   "metadata": {},
   "source": [
    "# Clean Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538377",
   "metadata": {},
   "source": [
    "## Import and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98909075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import json\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Neural Network Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 80\n",
    "VOCAB_SIZE = 500  # Prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e177a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = 'cleaned_data'\n",
    "train_file = 'train_clean.csv'\n",
    "valid_file = 'validation_clean.csv'\n",
    "test_file = 'test_clean.csv'\n",
    "path_to_train = os.path.join(curr_dir, data_dir, train_file)\n",
    "path_to_valid = os.path.join(curr_dir, data_dir, valid_file)\n",
    "path_to_test = os.path.join(curr_dir, data_dir, test_file)\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "valid_df = pd.read_csv(path_to_valid)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "valid_df = pd.DataFrame(valid_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1c950",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce9b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# suboptimal_example is too noisy will thus be dropped\n",
    "text_cols = [\"tasks_use_model\", \"verify_method\"]\n",
    "numeric_cols = [\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                \"reference_expectation\", \"verify_frequency\"]\n",
    "binary_cols = [c for c in train_df.columns if \"task_types\" in c]\n",
    "\n",
    "best_task_cols = [c for c in train_df.columns if \"best_task_types\" in c]\n",
    "suboptimal_task_cols = [c for c in train_df.columns if \"suboptimal_task_types\" in c]\n",
    "\n",
    "def add_task_sum(df):\n",
    "    df['best_task_count'] = df[best_task_cols].sum(axis=1)\n",
    "    df['suboptimal_task_count'] = df[suboptimal_task_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = add_task_sum(train_df)\n",
    "valid_df = add_task_sum(valid_df)\n",
    "test_df = add_task_sum(test_df)\n",
    "\n",
    "def clean_text(s):\n",
    "    # Convert input to string, handling NaNs, floats, etc.\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s)  \n",
    "    \n",
    "    # Your original cleaning logic\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in text_cols:\n",
    "    # Use .astype(str) on the column before applying for extra safety, \n",
    "    # or just use the improved clean_text function.\n",
    "    train_df[col] = train_df[col].apply(clean_text) \n",
    "    valid_df[col] = valid_df[col].apply(clean_text)\n",
    "    test_df[col] = test_df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df47c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = combine_text(train_df)\n",
    "valid_df = combine_text(valid_df)\n",
    "test_df = combine_text(test_df)\n",
    "\n",
    "# combined_text = train_df[\"full_text\"] + test_df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36d816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary (Top 300 words only)\n",
    "word_counts = Counter(train_df[\"full_text\"].str.cat(sep=\" \").split())\n",
    "vocab_list = sorted([w for w, c in word_counts.most_common(VOCAB_SIZE)])\n",
    "vocab_map = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e787a1",
   "metadata": {},
   "source": [
    "## SKIP THESE CODE BLOCKS. KEEPING IT STILL JUST FOR REFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "458d5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_text_to_bow(text_series, vocab_vector):\n",
    "#     \"\"\"\n",
    "#     Converts a pandas Series of text into a raw Bag-of-Words count NumPy array\n",
    "#     based on a provided vocabulary.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Create a dictionary map for fast vocabulary lookup\n",
    "#     # This maps the word to its column index in the final matrix\n",
    "#     vocab_map = {word: i for i, word in enumerate(vocab_vector)}\n",
    "#     vocab_size = len(vocab_vector)\n",
    "#     num_documents = len(text_series)\n",
    "    \n",
    "#     # Initialize the count matrix (BoW)\n",
    "#     # Using integer type for simple counts\n",
    "#     X_bow = np.zeros((num_documents, vocab_size), dtype=np.int32)\n",
    "    \n",
    "#     # 2. Fill the BoW count matrix\n",
    "#     for doc_index, document in enumerate(text_series):\n",
    "#         # The text is assumed to be cleaned and lowercased already\n",
    "#         words = document.split()\n",
    "        \n",
    "#         for word in words:\n",
    "#             if word in vocab_map:\n",
    "#                 word_index = vocab_map[word]\n",
    "#                 # Increment the count for this word in this document\n",
    "#                 X_bow[doc_index, word_index] += 1\n",
    "                \n",
    "#     return X_bow\n",
    "\n",
    "# X_train_bow = encode_text_to_bow(train_df['full_text'], vocab_vector)\n",
    "# X_valid_bow = encode_text_to_bow(valid_df['full_text'], vocab_vector)\n",
    "# X_test_bow = encode_text_to_bow(test_df['full_text'], vocab_vector)\n",
    "\n",
    "# print(f\"Shape of Training BoW Matrix: {X_train_bow.shape}\")\n",
    "# print(f\"Example of first row (document counts): {X_train_bow[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d0377fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded = pd.DataFrame([train_df['academic_use_likelihood'], \n",
    "#                         train_df['suboptimal_frequency'], \n",
    "#                         train_df['reference_expectation'],\n",
    "#                         train_df['verify_frequency']]).transpose()\n",
    "\n",
    "# valid_encoded = pd.DataFrame([valid_df['academic_use_likelihood'], \n",
    "#                         valid_df['suboptimal_frequency'], \n",
    "#                         valid_df['reference_expectation'],\n",
    "#                         valid_df['verify_frequency']]).transpose()\n",
    "\n",
    "# test_encoded = pd.DataFrame([test_df['academic_use_likelihood'], \n",
    "#                         test_df['suboptimal_frequency'], \n",
    "#                         test_df['reference_expectation'],\n",
    "#                         test_df['verify_frequency']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac8cef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded = pd.concat([train_encoded, pd.DataFrame(X_train_bow)],ignore_index=True, sort=False, axis=1)\n",
    "# valid_encoded = pd.concat([valid_encoded, pd.DataFrame(X_valid_bow)],ignore_index=True, sort=False, axis=1)\n",
    "# test_encoded = pd.concat([test_encoded, pd.DataFrame(X_test_bow)],ignore_index=True, sort=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6a3eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_t = np.stack([train_df['label']], axis=1).reshape(-1)\n",
    "# valid_t = np.stack([valid_df['label']], axis=1).reshape(-1)\n",
    "# test_t = np.stack([test_df['label']], axis=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df99a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target\n",
    "# X_num_cat = train_df[num_cols + cat_cols].copy()\n",
    "# y = train_df[target_col].values\n",
    "\n",
    "# # Scale numeric columns (on full dataset)\n",
    "# scaler = StandardScaler()\n",
    "# if num_cols:\n",
    "#     X_num_scaled = scaler.fit_transform(X_num_cat[num_cols])\n",
    "#     X_num_cat[num_cols] = X_num_scaled\n",
    "\n",
    "# X_num_cat = X_num_cat.to_numpy().astype(np.float32)  # numeric + cat\n",
    "# X_bow = bow_matrix.astype(np.float32)\n",
    "\n",
    "# # Final feature matrix: concat [numeric+cat, BoW]\n",
    "# X = np.concatenate([X_num_cat, X_bow], axis=1)\n",
    "# y = y.astype(np.int64)\n",
    "\n",
    "# N, input_dim = X.shape\n",
    "# num_classes = len(np.unique(y))\n",
    "\n",
    "# print(\"X shape:\", X.shape)\n",
    "# print(\"y shape:\", y.shape)\n",
    "# print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f73a6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_num_cat = valid_df[num_cols + cat_cols].copy()\n",
    "\n",
    "# # Scale numeric columns with the same scaler\n",
    "# if num_cols:\n",
    "#     X_val_num = scaler.transform(X_val_num_cat[num_cols])\n",
    "#     X_val_num_cat[num_cols] = X_val_num\n",
    "\n",
    "# X_val_num_cat = X_val_num_cat.to_numpy().astype(np.float32)\n",
    "# X_val = np.concatenate([X_val_num_cat, bow_matrix_val], axis=1)\n",
    "\n",
    "# y_val = valid_df[target_col].astype(np.int64).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be9c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# train_ds = TensorDataset(\n",
    "#     torch.from_numpy(train_encoded.to_numpy().astype(np.float32)),\n",
    "#     torch.from_numpy(train_t.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02589c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# val_ds = TensorDataset(\n",
    "#     torch.from_numpy(valid_encoded.to_numpy().astype(np.float32)),\n",
    "#     torch.from_numpy(valid_t.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b08ba207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# test_ds = TensorDataset(\n",
    "#     torch.from_numpy(test_encoded.to_numpy().astype(np.float32)),\n",
    "#     torch.from_numpy(test_t.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c2663f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim=train_encoded.to_numpy().shape[1]\n",
    "# num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17189e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e9e3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLPBoW(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=64, num_classes=3, dropout_p=0.3):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         # self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.out = nn.Linear(hidden_dim, num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         # x = self.relu(self.fc2(x))\n",
    "#         # x = self.dropout(x)\n",
    "#         x = self.out(x)\n",
    "#         return x\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MLPBoW(input_dim=input_dim, hidden_dim=64, num_classes=num_classes, dropout_p=0.3).to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20c18958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# def evaluate(loader):\n",
    "#     model.eval()\n",
    "#     correct, total, running_loss = 0, 0, 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb = xb.to(device)\n",
    "#             yb = yb.to(device)\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "#             running_loss += loss.item() * xb.size(0)\n",
    "#             preds = logits.argmax(dim=1)\n",
    "#             correct += (preds == yb).sum().item()\n",
    "#             total += yb.size(0)\n",
    "#     return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2af2e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "\n",
    "# num_epochs = 100\n",
    "# patience = 10\n",
    "# min_delta = 0.0\n",
    "\n",
    "# best_val_loss = float(\"inf\")\n",
    "# best_epoch = 0\n",
    "# epochs_no_improve = 0\n",
    "# best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb = xb.to(device)\n",
    "#         yb = yb.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(xb)\n",
    "#         loss = criterion(logits, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "#     # === After epoch: evaluate ===\n",
    "#     train_loss, train_acc = evaluate(train_loader)\n",
    "#     val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "#     history[\"train_loss\"].append(train_loss)\n",
    "#     history[\"train_acc\"].append(train_acc)\n",
    "#     history[\"val_loss\"].append(val_loss)\n",
    "#     history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "#     if epoch % 5 == 0 or epoch == 1:\n",
    "#         print(\n",
    "#             f\"Epoch {epoch:3d} | \"\n",
    "#             f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
    "#             f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\"\n",
    "#         )\n",
    "\n",
    "# # restore best model\n",
    "# model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5361de4",
   "metadata": {},
   "source": [
    "## END SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe6743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(df, text_series, vocab_map, is_train=True):\n",
    "    \n",
    "    # Extract original numeric cols\n",
    "    orig_nums = df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                    \"reference_expectation\", \"verify_frequency\"]].values\n",
    "    scaled_orig = (orig_nums - 3.0) / 1.2\n",
    "    \n",
    "    # Scale the new feature separately\n",
    "    task_count = df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "    scaled_count = (task_count - 2.0) / 1.5 \n",
    "\n",
    "    X_num = np.hstack([scaled_orig, scaled_count])\n",
    "    \n",
    "    # 2. Binary Features\n",
    "    X_bin = df[binary_cols].values\n",
    "    \n",
    "    # 3. Bag of Words (Log Scaled)\n",
    "    X_bow = np.zeros((len(df), len(vocab_map)), dtype=np.float32)\n",
    "    for i, text in enumerate(text_series):\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            if w in vocab_map:\n",
    "                X_bow[i, vocab_map[w]] += 1\n",
    "    X_bow = np.log1p(X_bow)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.hstack([X_num, X_bin, X_bow]).astype(np.float32)\n",
    "    \n",
    "    if is_train or 'label' in df.columns:\n",
    "        y = df['label'].values.astype(np.int64)\n",
    "        return X, y\n",
    "    return X, None\n",
    "\n",
    "X_train, y_train = get_features_labels(train_df, train_df[\"full_text\"], vocab_map)\n",
    "X_valid, y_valid = get_features_labels(valid_df, valid_df[\"full_text\"], vocab_map)\n",
    "X_test, y_test = get_features_labels(test_df, test_df[\"full_text\"], vocab_map)\n",
    "\n",
    "# Loaders\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_tensor = torch.utils.data.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "valid_loader = torch.utils.data.DataLoader(valid_tensor, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c5b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4) # High dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2372cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Accuracy: 0.6904761791229248 | Best Learning Rate: 3e-06\n"
     ]
    }
   ],
   "source": [
    "def train_model(lr, step_size):\n",
    "    model = MLP(input_dim=X_train.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.5)\n",
    "    \n",
    "\n",
    "    # 1. Setup tracking for the best model\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # Initialize with current weights\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # --- TRAIN PHASE ---\n",
    "        model.train() # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_correct += (preds == y_batch).sum().item()\n",
    "            train_total += y_batch.size(0)\n",
    "            \n",
    "        # Calculate average training loss and accuracy\n",
    "        epoch_train_loss = train_loss / train_total\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval() # Set model to evaluation mode (critical for Dropout/BatchNorm)\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient calculation for speed\n",
    "            for X_valid, y_valid in valid_loader:\n",
    "                # Forward pass only\n",
    "                outputs = model(X_valid)\n",
    "                loss = criterion(outputs, y_valid)\n",
    "                \n",
    "                # Track validation metrics\n",
    "                val_loss += loss.item() * X_valid.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (preds == y_valid).sum().item()\n",
    "                val_total += y_valid.size(0)\n",
    "                \n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # # Step the scheduler (if you are using one)\n",
    "        # if 'scheduler' in locals():\n",
    "        #     scheduler.step()\n",
    "\n",
    "        # --- CHECKPOINTING ---\n",
    "        # If this epoch's validation accuracy is better than the best we've seen, save it!\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict()) # Save a deep copy of weights\n",
    "            # print(f\"Epoch {epoch+1:2d} | Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} *NEW BEST*\")\n",
    "    #     elif (epoch+1) % 5 == 0:\n",
    "    #         # Print status every 5 epochs even if not a new best\n",
    "    #         print(f\"Epoch {epoch+1:2d} | Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # Final Evaluation\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(torch.tensor(X_test))\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc = (preds == torch.tensor(y_test)).float().mean()\n",
    "            # print(f\"\\nFinal Test Accuracy: {acc.item():.4f}\")\n",
    "    return acc, lr\n",
    "\n",
    "best_acc = 0\n",
    "best_lr = 0\n",
    "learning_rates = [0.002, 0.0005, 0.000003]\n",
    "for lr in learning_rates:\n",
    "    acc, lr = train_model(lr, 20)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_lr = lr\n",
    "print(f\"Best Test Accuracy: {best_acc} | Best Learning Rate: {lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc5807",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b8855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 0.6587\n"
     ]
    }
   ],
   "source": [
    "# # Final Evaluation\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     logits = model(torch.tensor(X_test))\n",
    "#     preds = torch.argmax(logits, dim=1)\n",
    "#     acc = (preds == torch.tensor(y_test)).float().mean()\n",
    "#     print(f\"\\nFinal Test Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef36733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

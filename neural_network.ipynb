{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0992197",
   "metadata": {},
   "source": [
    "# Clean Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538377",
   "metadata": {},
   "source": [
    "## Import and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98909075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Neural Network Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 80\n",
    "VOCAB_SIZE = 500  # Prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e177a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = 'cleaned_data'\n",
    "train_file = 'train_clean.csv'\n",
    "valid_file = 'validation_clean.csv'\n",
    "test_file = 'test_clean.csv'\n",
    "path_to_train = os.path.join(curr_dir, data_dir, train_file)\n",
    "path_to_valid = os.path.join(curr_dir, data_dir, valid_file)\n",
    "path_to_test = os.path.join(curr_dir, data_dir, test_file)\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "valid_df = pd.read_csv(path_to_valid)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "valid_df = pd.DataFrame(valid_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1c950",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# suboptimal_example is too noisy will thus be dropped\n",
    "text_cols = [\"tasks_use_model\", \"verify_method\"]\n",
    "numeric_cols = [\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                \"reference_expectation\", \"verify_frequency\"]\n",
    "binary_cols = [c for c in train_df.columns if \"task_types\" in c]\n",
    "\n",
    "best_task_cols = [c for c in train_df.columns if \"best_task_types\" in c]\n",
    "suboptimal_task_cols = [c for c in train_df.columns if \"suboptimal_task_types\" in c]\n",
    "\n",
    "def add_task_sum(df):\n",
    "    df['best_task_count'] = df[best_task_cols].sum(axis=1)\n",
    "    df['suboptimal_task_count'] = df[suboptimal_task_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = add_task_sum(train_df)\n",
    "valid_df = add_task_sum(valid_df)\n",
    "test_df = add_task_sum(test_df)\n",
    "\n",
    "def clean_text(s):\n",
    "    # Convert input to string, handling NaNs, floats, etc.\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s)  \n",
    "    \n",
    "    # Your original cleaning logic\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in text_cols:\n",
    "    # Use .astype(str) on the column before applying for extra safety, \n",
    "    # or just use the improved clean_text function.\n",
    "    train_df[col] = train_df[col].apply(clean_text) \n",
    "    valid_df[col] = valid_df[col].apply(clean_text)\n",
    "    test_df[col] = test_df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df47c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = combine_text(train_df)\n",
    "valid_df = combine_text(valid_df)\n",
    "test_df = combine_text(test_df)\n",
    "\n",
    "# combined_text = train_df[\"full_text\"] + test_df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary (Top 300 words only)\n",
    "word_counts = Counter(train_df[\"full_text\"].str.cat(sep=\" \").split())\n",
    "vocab_list = sorted([w for w, c in word_counts.most_common(VOCAB_SIZE)])\n",
    "vocab_map = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Fit Scalers on TRAIN data only\n",
    "scaler_num = StandardScaler()\n",
    "scaler_count = StandardScaler()\n",
    "\n",
    "# Extract raw data first to fit scalers\n",
    "train_orig_nums = train_df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                            \"reference_expectation\", \"verify_frequency\"]].values\n",
    "train_task_count = train_df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "\n",
    "scaler_num.fit(train_orig_nums)\n",
    "scaler_count.fit(train_task_count)\n",
    "\n",
    "def get_features_labels_robust(df, text_series, vocab_map, scaler_num, scaler_count):\n",
    "    # 1. Numeric Features (Use the fitted scalers)\n",
    "    orig_nums = df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                    \"reference_expectation\", \"verify_frequency\"]].values\n",
    "    # Fill NaNs with 0 or mean before scaling to prevent errors\n",
    "    orig_nums = np.nan_to_num(orig_nums) \n",
    "    scaled_orig = scaler_num.transform(orig_nums)\n",
    "    \n",
    "    task_count = df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "    task_count = np.nan_to_num(task_count)\n",
    "    scaled_count = scaler_count.transform(task_count)\n",
    "\n",
    "    X_num = np.hstack([scaled_orig, scaled_count])\n",
    "    \n",
    "    # 2. Binary Features (Ensure columns exist, fill missing with 0)\n",
    "    # Uses the global 'binary_cols' list from the training setup\n",
    "    try:\n",
    "        X_bin = df[binary_cols].fillna(0).values\n",
    "    except KeyError:\n",
    "        # Create missing columns if they don't exist in test\n",
    "        for c in binary_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = 0\n",
    "        X_bin = df[binary_cols].values\n",
    "    \n",
    "    # 3. Bag of Words\n",
    "    X_bow = np.zeros((len(df), len(vocab_map)), dtype=np.float32)\n",
    "    for i, text in enumerate(text_series):\n",
    "        words = str(text).split() # Ensure string\n",
    "        for w in words:\n",
    "            if w in vocab_map:\n",
    "                X_bow[i, vocab_map[w]] += 1\n",
    "    X_bow = np.log1p(X_bow)\n",
    "    \n",
    "    X = np.hstack([X_num, X_bin, X_bow]).astype(np.float32)\n",
    "    \n",
    "    y = None\n",
    "    if 'label' in df.columns:\n",
    "        y = df['label'].values.astype(np.int64)\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Re-generate datasets using the robust function\n",
    "X_train, y_train = get_features_labels_robust(train_df, train_df[\"full_text\"], vocab_map, scaler_num, scaler_count)\n",
    "X_valid, y_valid = get_features_labels_robust(valid_df, valid_df[\"full_text\"], vocab_map, scaler_num, scaler_count)\n",
    "X_test, y_test = get_features_labels_robust(test_df, test_df[\"full_text\"], vocab_map, scaler_num, scaler_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc98c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_tensor = torch.utils.data.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "valid_loader = torch.utils.data.DataLoader(valid_tensor, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48484b4c",
   "metadata": {},
   "source": [
    "## Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8e742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[128, 64], dropout_rate=0.4, output_dim=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_dim = h_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c3432ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(params, train_loader, valid_loader, input_dim, patience=8):\n",
    "    model = MLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=params['hidden_layers'],\n",
    "        dropout_rate=params['dropout']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for X_b, y_b in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_b)\n",
    "            loss = criterion(outputs, y_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_v, y_v in valid_loader:\n",
    "                outputs = model(X_v)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == y_v).sum().item()\n",
    "                total += y_v.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Early Stopping Logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0 \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "            \n",
    "    return best_val_acc, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9736421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 48 configurations...\n",
      "LR       | Layers          | Drop  | WD       | Val Acc \n",
      "------------------------------------------------------------\n",
      "0.001    | [128, 64]       | 0.3   | 0.001    | 0.7317\n",
      "0.001    | [128, 64]       | 0.3   | 0.0001   | 0.7317\n",
      "0.001    | [128, 64]       | 0.5   | 0.001    | 0.7236\n",
      "0.001    | [128, 64]       | 0.5   | 0.0001   | 0.7236\n",
      "0.001    | [256, 128]      | 0.3   | 0.001    | 0.7154\n",
      "0.001    | [256, 128]      | 0.3   | 0.0001   | 0.7317\n",
      "0.001    | [256, 128]      | 0.5   | 0.001    | 0.7236\n",
      "0.001    | [256, 128]      | 0.5   | 0.0001   | 0.7317\n",
      "0.001    | [128, 64, 32]   | 0.3   | 0.001    | 0.7154\n",
      "0.001    | [128, 64, 32]   | 0.3   | 0.0001   | 0.6992\n",
      "0.001    | [128, 64, 32]   | 0.5   | 0.001    | 0.7480\n",
      "0.001    | [128, 64, 32]   | 0.5   | 0.0001   | 0.7317\n",
      "0.001    | [64]            | 0.3   | 0.001    | 0.7561\n",
      "0.001    | [64]            | 0.3   | 0.0001   | 0.7561\n",
      "0.001    | [64]            | 0.5   | 0.001    | 0.7480\n",
      "0.001    | [64]            | 0.5   | 0.0001   | 0.7480\n",
      "0.0005   | [128, 64]       | 0.3   | 0.001    | 0.7561\n",
      "0.0005   | [128, 64]       | 0.3   | 0.0001   | 0.7317\n",
      "0.0005   | [128, 64]       | 0.5   | 0.001    | 0.7480\n",
      "0.0005   | [128, 64]       | 0.5   | 0.0001   | 0.7480\n",
      "0.0005   | [256, 128]      | 0.3   | 0.001    | 0.7561\n",
      "0.0005   | [256, 128]      | 0.3   | 0.0001   | 0.7236\n",
      "0.0005   | [256, 128]      | 0.5   | 0.001    | 0.7561\n",
      "0.0005   | [256, 128]      | 0.5   | 0.0001   | 0.7154\n",
      "0.0005   | [128, 64, 32]   | 0.3   | 0.001    | 0.7398\n",
      "0.0005   | [128, 64, 32]   | 0.3   | 0.0001   | 0.7154\n",
      "0.0005   | [128, 64, 32]   | 0.5   | 0.001    | 0.6992\n",
      "0.0005   | [128, 64, 32]   | 0.5   | 0.0001   | 0.7398\n",
      "0.0005   | [64]            | 0.3   | 0.001    | 0.7724\n",
      "0.0005   | [64]            | 0.3   | 0.0001   | 0.7480\n",
      "0.0005   | [64]            | 0.5   | 0.001    | 0.7642\n",
      "0.0005   | [64]            | 0.5   | 0.0001   | 0.7236\n",
      "0.0001   | [128, 64]       | 0.3   | 0.001    | 0.7154\n",
      "0.0001   | [128, 64]       | 0.3   | 0.0001   | 0.7073\n",
      "0.0001   | [128, 64]       | 0.5   | 0.001    | 0.6911\n",
      "0.0001   | [128, 64]       | 0.5   | 0.0001   | 0.6016\n",
      "0.0001   | [256, 128]      | 0.3   | 0.001    | 0.7398\n",
      "0.0001   | [256, 128]      | 0.3   | 0.0001   | 0.7480\n",
      "0.0001   | [256, 128]      | 0.5   | 0.001    | 0.7073\n",
      "0.0001   | [256, 128]      | 0.5   | 0.0001   | 0.7480\n",
      "0.0001   | [128, 64, 32]   | 0.3   | 0.001    | 0.5528\n",
      "0.0001   | [128, 64, 32]   | 0.3   | 0.0001   | 0.5854\n",
      "0.0001   | [128, 64, 32]   | 0.5   | 0.001    | 0.7561\n",
      "0.0001   | [128, 64, 32]   | 0.5   | 0.0001   | 0.3333\n",
      "0.0001   | [64]            | 0.3   | 0.001    | 0.7398\n",
      "0.0001   | [64]            | 0.3   | 0.0001   | 0.7642\n",
      "0.0001   | [64]            | 0.5   | 0.001    | 0.6341\n",
      "0.0001   | [64]            | 0.5   | 0.0001   | 0.7154\n",
      "------------------------------------------------------------\n",
      "Best Accuracy: 0.7724\n",
      "Best Config: {'lr': 0.0005, 'hidden_layers': [64], 'dropout': 0.3, 'weight_decay': 0.001, 'epochs': 80}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter Search Space\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'hidden_layers': [\n",
    "        [128, 64],       # Original structure\n",
    "        [256, 128],      # Wider\n",
    "        [128, 64, 32],   # Deeper\n",
    "        [64]             # Simpler\n",
    "    ],\n",
    "    'dropout': [0.3, 0.5],\n",
    "    'weight_decay': [1e-3, 1e-4],\n",
    "    'epochs': [80] # High cap, controlled by early stopping\n",
    "}\n",
    "\n",
    "best_overall_acc = 0.0\n",
    "best_config = {}\n",
    "best_weights = None\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Generate combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"Testing {len(combinations)} configurations...\")\n",
    "print(f\"{'LR':<8} | {'Layers':<15} | {'Drop':<5} | {'WD':<8} | {'Val Acc':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for params in combinations:\n",
    "    val_acc, weights = grid_search(params, train_loader, valid_loader, input_dim)\n",
    "    \n",
    "    layer_str = str(params['hidden_layers'])\n",
    "    print(f\"{params['lr']:<8} | {layer_str:<15} | {params['dropout']:<5} | {params['weight_decay']:<8} | {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_overall_acc:\n",
    "        best_overall_acc = val_acc\n",
    "        best_config = params\n",
    "        best_weights = weights\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Best Accuracy: {best_overall_acc:.4f}\")\n",
    "print(f\"Best Config: {best_config}\")\n",
    "\n",
    "# Load best weights into a model instance for final testing\n",
    "model = MLP(input_dim, best_config['hidden_layers'], best_config['dropout'])\n",
    "model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b16df",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22f82991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=522, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7724\n",
      "Validation Macro F1: 0.7707\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_v, y_v in loader:\n",
    "            outputs = model(X_v)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_v.cpu().numpy())\n",
    "            \n",
    "    # Compute Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "# Run evaluation\n",
    "v_acc, v_f1 = evaluate(model, valid_loader)\n",
    "print(f\"Validation Accuracy: {v_acc:.4f}\")\n",
    "print(f\"Validation Macro F1: {v_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c96935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Accuracy: 0.7724\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ChatGPT       0.80      0.88      0.84        41\n",
      "      Claude       0.80      0.68      0.74        41\n",
      "      Gemini       0.72      0.76      0.74        41\n",
      "\n",
      "    accuracy                           0.77       123\n",
      "   macro avg       0.77      0.77      0.77       123\n",
      "weighted avg       0.77      0.77      0.77       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "with torch.no_grad():\n",
    "    if not isinstance(X_test, torch.Tensor):\n",
    "        X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    else:\n",
    "        X_valid_tensor = X_valid\n",
    "        \n",
    "    logits = model(X_valid_tensor)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "print(f\"Validation Set Accuracy: {acc:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, preds, target_names=[\"ChatGPT\", \"Claude\", \"Gemini\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d05225ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7063\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ChatGPT       0.82      0.86      0.84        42\n",
      "      Claude       0.62      0.76      0.68        42\n",
      "      Gemini       0.70      0.50      0.58        42\n",
      "\n",
      "    accuracy                           0.71       126\n",
      "   macro avg       0.71      0.71      0.70       126\n",
      "weighted avg       0.71      0.71      0.70       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "with torch.no_grad():\n",
    "    if not isinstance(X_test, torch.Tensor):\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        X_test_tensor = X_test\n",
    "        \n",
    "    logits = model(X_test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Test Set Accuracy: {acc:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, preds, target_names=[\"ChatGPT\", \"Claude\", \"Gemini\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655d72a",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb84b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete: model_artifacts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. Extract Model Weights & Biases\n",
    "# We transpose weights (.T) so the math becomes (input @ weights + bias)\n",
    "# This is standard for NumPy inference.\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "for key, param in model.state_dict().items():\n",
    "    if 'weight' in key:\n",
    "        weights.append(param.cpu().detach().numpy().T.tolist())\n",
    "    elif 'bias' in key:\n",
    "        biases.append(param.cpu().detach().numpy().tolist())\n",
    "\n",
    "# 2. Extract Scaler Statistics (Critical for correct input scaling)\n",
    "# Check if scalers are fitted; if using the manual method, you might need to hardcode these\n",
    "scaler_data = {\n",
    "    \"num_mean\": scaler_num.mean_.tolist(),\n",
    "    \"num_scale\": scaler_num.scale_.tolist(),\n",
    "    \"count_mean\": scaler_count.mean_.tolist(),\n",
    "    \"count_scale\": scaler_count.scale_.tolist()\n",
    "}\n",
    "\n",
    "# 3. Bundle Everything\n",
    "artifacts = {\n",
    "    \"weights\": weights,\n",
    "    \"biases\": biases,\n",
    "    \"vocab_map\": vocab_map,\n",
    "    \"binary_cols\": binary_cols, # List of binary column names\n",
    "    \"scalers\": scaler_data\n",
    "}\n",
    "\n",
    "with open('model_artifacts.json', 'w') as f:\n",
    "    json.dump(artifacts, f)\n",
    "\n",
    "print(\"Export complete: model_artifacts.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

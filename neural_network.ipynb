{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0992197",
   "metadata": {},
   "source": [
    "# Clean Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538377",
   "metadata": {},
   "source": [
    "## Import and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "98909075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import json\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Neural Network Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.002\n",
    "EPOCHS = 80\n",
    "VOCAB_SIZE = 300  # Prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e177a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = 'cleaned_data'\n",
    "train_file = 'train_clean.csv'\n",
    "valid_file = 'validation_clean.csv'\n",
    "test_file = 'test_clean.csv'\n",
    "path_to_train = os.path.join(curr_dir, data_dir, train_file)\n",
    "path_to_valid = os.path.join(curr_dir, data_dir, valid_file)\n",
    "path_to_test = os.path.join(curr_dir, data_dir, test_file)\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "valid_df = pd.read_csv(path_to_valid)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "valid_df = pd.DataFrame(valid_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1c950",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9fb1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, valid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ce9b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# suboptimal_example is too noisy will thus be dropped\n",
    "text_cols = [\"tasks_use_model\", \"verify_method\"]\n",
    "numeric_cols = [\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                \"reference_expectation\", \"verify_frequency\"]\n",
    "binary_cols = [c for c in train_df.columns if \"task_types\" in c]\n",
    "\n",
    "best_task_cols = [c for c in train_df.columns if \"best_task_types\" in c]\n",
    "suboptimal_task_cols = [c for c in train_df.columns if \"suboptimal_task_types\" in c]\n",
    "\n",
    "def add_task_sum(df):\n",
    "    df['best_task_count'] = df[best_task_cols].sum(axis=1)\n",
    "    df['suboptimal_task_count'] = df[suboptimal_task_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = add_task_sum(train_df)\n",
    "test_df = add_task_sum(test_df)\n",
    "\n",
    "def clean_text(s):\n",
    "    # Convert input to string, handling NaNs, floats, etc.\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s)  \n",
    "    \n",
    "    # Your original cleaning logic\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in text_cols:\n",
    "    # Use .astype(str) on the column before applying for extra safety, \n",
    "    # or just use the improved clean_text function.\n",
    "    train_df[col] = train_df[col].apply(clean_text) \n",
    "    test_df[col] = test_df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df47c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = combine_text(train_df)\n",
    "test_df = combine_text(test_df)\n",
    "\n",
    "# combined_text = train_df[\"full_text\"] + test_df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36d816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary (Top 300 words only)\n",
    "word_counts = Counter(train_df[\"full_text\"].str.cat(sep=\" \").split())\n",
    "vocab_list = sorted([w for w, c in word_counts.most_common(VOCAB_SIZE)])\n",
    "vocab_map = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e787a1",
   "metadata": {},
   "source": [
    "## SKIP THESE CODE BLOCKS. KEEPING IT STILL JUST FOR REFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training BoW Matrix: (576, 500)\n",
      "Example of first row (document counts): [0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# def encode_text_to_bow(text_series, vocab_vector):\n",
    "#     \"\"\"\n",
    "#     Converts a pandas Series of text into a raw Bag-of-Words count NumPy array\n",
    "#     based on a provided vocabulary.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Create a dictionary map for fast vocabulary lookup\n",
    "#     # This maps the word to its column index in the final matrix\n",
    "#     vocab_map = {word: i for i, word in enumerate(vocab_vector)}\n",
    "#     vocab_size = len(vocab_vector)\n",
    "#     num_documents = len(text_series)\n",
    "    \n",
    "#     # Initialize the count matrix (BoW)\n",
    "#     # Using integer type for simple counts\n",
    "#     X_bow = np.zeros((num_documents, vocab_size), dtype=np.int32)\n",
    "    \n",
    "#     # 2. Fill the BoW count matrix\n",
    "#     for doc_index, document in enumerate(text_series):\n",
    "#         # The text is assumed to be cleaned and lowercased already\n",
    "#         words = document.split()\n",
    "        \n",
    "#         for word in words:\n",
    "#             if word in vocab_map:\n",
    "#                 word_index = vocab_map[word]\n",
    "#                 # Increment the count for this word in this document\n",
    "#                 X_bow[doc_index, word_index] += 1\n",
    "                \n",
    "#     return X_bow\n",
    "\n",
    "# X_train_bow = encode_text_to_bow(train_df['full_text'], vocab_vector)\n",
    "# X_valid_bow = encode_text_to_bow(valid_df['full_text'], vocab_vector)\n",
    "# X_test_bow = encode_text_to_bow(test_df['full_text'], vocab_vector)\n",
    "\n",
    "# print(f\"Shape of Training BoW Matrix: {X_train_bow.shape}\")\n",
    "# print(f\"Example of first row (document counts): {X_train_bow[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0377fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded = pd.DataFrame([train_df['academic_use_likelihood'], \n",
    "#                         train_df['suboptimal_frequency'], \n",
    "#                         train_df['reference_expectation'],\n",
    "#                         train_df['verify_frequency']]).transpose()\n",
    "\n",
    "# valid_encoded = pd.DataFrame([valid_df['academic_use_likelihood'], \n",
    "#                         valid_df['suboptimal_frequency'], \n",
    "#                         valid_df['reference_expectation'],\n",
    "#                         valid_df['verify_frequency']]).transpose()\n",
    "\n",
    "# test_encoded = pd.DataFrame([test_df['academic_use_likelihood'], \n",
    "#                         test_df['suboptimal_frequency'], \n",
    "#                         test_df['reference_expectation'],\n",
    "#                         test_df['verify_frequency']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded = pd.concat([train_encoded, pd.DataFrame(X_train_bow)],ignore_index=True, sort=False, axis=1)\n",
    "# valid_encoded = pd.concat([valid_encoded, pd.DataFrame(X_valid_bow)],ignore_index=True, sort=False, axis=1)\n",
    "# test_encoded = pd.concat([test_encoded, pd.DataFrame(X_test_bow)],ignore_index=True, sort=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_t = np.stack([train_df['label']], axis=1).reshape(-1)\n",
    "# valid_t = np.stack([valid_df['label']], axis=1).reshape(-1)\n",
    "# test_t = np.stack([test_df['label']], axis=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df99a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target\n",
    "# X_num_cat = train_df[num_cols + cat_cols].copy()\n",
    "# y = train_df[target_col].values\n",
    "\n",
    "# # Scale numeric columns (on full dataset)\n",
    "# scaler = StandardScaler()\n",
    "# if num_cols:\n",
    "#     X_num_scaled = scaler.fit_transform(X_num_cat[num_cols])\n",
    "#     X_num_cat[num_cols] = X_num_scaled\n",
    "\n",
    "# X_num_cat = X_num_cat.to_numpy().astype(np.float32)  # numeric + cat\n",
    "# X_bow = bow_matrix.astype(np.float32)\n",
    "\n",
    "# # Final feature matrix: concat [numeric+cat, BoW]\n",
    "# X = np.concatenate([X_num_cat, X_bow], axis=1)\n",
    "# y = y.astype(np.int64)\n",
    "\n",
    "# N, input_dim = X.shape\n",
    "# num_classes = len(np.unique(y))\n",
    "\n",
    "# print(\"X shape:\", X.shape)\n",
    "# print(\"y shape:\", y.shape)\n",
    "# print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f73a6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_num_cat = valid_df[num_cols + cat_cols].copy()\n",
    "\n",
    "# # Scale numeric columns with the same scaler\n",
    "# if num_cols:\n",
    "#     X_val_num = scaler.transform(X_val_num_cat[num_cols])\n",
    "#     X_val_num_cat[num_cols] = X_val_num\n",
    "\n",
    "# X_val_num_cat = X_val_num_cat.to_numpy().astype(np.float32)\n",
    "# X_val = np.concatenate([X_val_num_cat, bow_matrix_val], axis=1)\n",
    "\n",
    "# y_val = valid_df[target_col].astype(np.int64).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# train_ds = TensorDataset(\n",
    "#     torch.from_numpy(train_encoded.to_numpy().astype(np.float32)),\n",
    "#     torch.from_numpy(train_t.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02589c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# val_ds = TensorDataset(\n",
    "#     torch.from_numpy(valid_encoded.to_numpy().astype(np.float32)),\n",
    "#     torch.from_numpy(valid_t.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ba207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# test_ds = TensorDataset(\n",
    "#     torch.from_numpy(test_encoded.to_numpy().astype(np.float32)),\n",
    "#     torch.from_numpy(test_t.astype(np.int64))\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2663f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim=train_encoded.to_numpy().shape[1]\n",
    "# num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17189e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 504])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e3865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPBoW(\n",
       "  (fc1): Linear(in_features=504, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class MLPBoW(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=64, num_classes=3, dropout_p=0.3):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         # self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.out = nn.Linear(hidden_dim, num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         # x = self.relu(self.fc2(x))\n",
    "#         # x = self.dropout(x)\n",
    "#         x = self.out(x)\n",
    "#         return x\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MLPBoW(input_dim=input_dim, hidden_dim=64, num_classes=num_classes, dropout_p=0.3).to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c18958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# def evaluate(loader):\n",
    "#     model.eval()\n",
    "#     correct, total, running_loss = 0, 0, 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb = xb.to(device)\n",
    "#             yb = yb.to(device)\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "#             running_loss += loss.item() * xb.size(0)\n",
    "#             preds = logits.argmax(dim=1)\n",
    "#             correct += (preds == yb).sum().item()\n",
    "#             total += yb.size(0)\n",
    "#     return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train_loss=1.0124 | train_acc=0.6545 | val_loss=1.0461 | val_acc=0.5772\n",
      "Epoch   5 | train_loss=0.5773 | train_acc=0.8368 | val_loss=0.8682 | val_acc=0.5935\n",
      "Epoch  10 | train_loss=0.2974 | train_acc=0.9288 | val_loss=0.9481 | val_acc=0.5772\n",
      "Epoch  15 | train_loss=0.1709 | train_acc=0.9705 | val_loss=1.2010 | val_acc=0.5691\n",
      "Epoch  20 | train_loss=0.1095 | train_acc=0.9792 | val_loss=1.4643 | val_acc=0.5691\n",
      "Epoch  25 | train_loss=0.0768 | train_acc=0.9809 | val_loss=1.7373 | val_acc=0.5447\n",
      "Epoch  30 | train_loss=0.0583 | train_acc=0.9878 | val_loss=1.9237 | val_acc=0.5366\n",
      "Epoch  35 | train_loss=0.0472 | train_acc=0.9896 | val_loss=2.1184 | val_acc=0.5041\n",
      "Epoch  40 | train_loss=0.0398 | train_acc=0.9878 | val_loss=2.2762 | val_acc=0.4959\n",
      "Epoch  45 | train_loss=0.0367 | train_acc=0.9896 | val_loss=2.3879 | val_acc=0.4878\n",
      "Epoch  50 | train_loss=0.0307 | train_acc=0.9931 | val_loss=2.5347 | val_acc=0.5041\n",
      "Epoch  55 | train_loss=0.0279 | train_acc=0.9913 | val_loss=2.6296 | val_acc=0.5041\n",
      "Epoch  60 | train_loss=0.0265 | train_acc=0.9931 | val_loss=2.7359 | val_acc=0.4797\n",
      "Epoch  65 | train_loss=0.0238 | train_acc=0.9931 | val_loss=2.8469 | val_acc=0.4959\n",
      "Epoch  70 | train_loss=0.0244 | train_acc=0.9913 | val_loss=2.9073 | val_acc=0.4878\n",
      "Epoch  75 | train_loss=0.0225 | train_acc=0.9913 | val_loss=3.1201 | val_acc=0.4797\n",
      "Epoch  80 | train_loss=0.0258 | train_acc=0.9931 | val_loss=3.2487 | val_acc=0.4797\n",
      "Epoch  85 | train_loss=0.0212 | train_acc=0.9896 | val_loss=3.2515 | val_acc=0.4959\n",
      "Epoch  90 | train_loss=0.0196 | train_acc=0.9931 | val_loss=3.3006 | val_acc=0.4634\n",
      "Epoch  95 | train_loss=0.0180 | train_acc=0.9948 | val_loss=3.3460 | val_acc=0.4797\n",
      "Epoch 100 | train_loss=0.0177 | train_acc=0.9931 | val_loss=3.3986 | val_acc=0.4878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import copy\n",
    "\n",
    "# num_epochs = 100\n",
    "# patience = 10\n",
    "# min_delta = 0.0\n",
    "\n",
    "# best_val_loss = float(\"inf\")\n",
    "# best_epoch = 0\n",
    "# epochs_no_improve = 0\n",
    "# best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb = xb.to(device)\n",
    "#         yb = yb.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(xb)\n",
    "#         loss = criterion(logits, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "#     # === After epoch: evaluate ===\n",
    "#     train_loss, train_acc = evaluate(train_loader)\n",
    "#     val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "#     history[\"train_loss\"].append(train_loss)\n",
    "#     history[\"train_acc\"].append(train_acc)\n",
    "#     history[\"val_loss\"].append(val_loss)\n",
    "#     history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "#     if epoch % 5 == 0 or epoch == 1:\n",
    "#         print(\n",
    "#             f\"Epoch {epoch:3d} | \"\n",
    "#             f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
    "#             f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\"\n",
    "#         )\n",
    "\n",
    "# # restore best model\n",
    "# model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5361de4",
   "metadata": {},
   "source": [
    "## END SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fe6743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(df, text_series, vocab_map, is_train=True):\n",
    "    \n",
    "    # Extract original numeric cols\n",
    "    orig_nums = df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                    \"reference_expectation\", \"verify_frequency\"]].values\n",
    "    scaled_orig = (orig_nums - 3.0) / 1.2\n",
    "    \n",
    "    # Scale the new feature separately\n",
    "    task_count = df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "    scaled_count = (task_count - 2.0) / 1.5 \n",
    "    \n",
    "    X_num = np.hstack([scaled_orig, scaled_count])\n",
    "    \n",
    "    # 2. Binary Features\n",
    "    X_bin = df[binary_cols].values\n",
    "    \n",
    "    # 3. Bag of Words (Log Scaled)\n",
    "    X_bow = np.zeros((len(df), len(vocab_map)), dtype=np.float32)\n",
    "    for i, text in enumerate(text_series):\n",
    "        words = text.split()\n",
    "        for w in words:\n",
    "            if w in vocab_map:\n",
    "                X_bow[i, vocab_map[w]] += 1\n",
    "    X_bow = np.log1p(X_bow)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.hstack([X_num, X_bin, X_bow]).astype(np.float32)\n",
    "    \n",
    "    if is_train or 'label' in df.columns:\n",
    "        y = df['label'].values.astype(np.int64)\n",
    "        return X, y\n",
    "    return X, None\n",
    "\n",
    "X_train, y_train = get_features_labels(train_df, train_df[\"full_text\"], vocab_map)\n",
    "X_test, y_test = get_features_labels(test_df, test_df[\"full_text\"], vocab_map)\n",
    "\n",
    "# Loaders\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4c5b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4) # High dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2372cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Loss: 0.5885 | Train Acc: 0.7582\n",
      "Epoch 10 | Loss: 0.4186 | Train Acc: 0.8484\n",
      "Epoch 15 | Loss: 0.2755 | Train Acc: 0.9070\n",
      "Epoch 20 | Loss: 0.1981 | Train Acc: 0.9328\n",
      "Epoch 25 | Loss: 0.1431 | Train Acc: 0.9557\n",
      "Epoch 30 | Loss: 0.1151 | Train Acc: 0.9642\n",
      "Epoch 35 | Loss: 0.0918 | Train Acc: 0.9714\n",
      "Epoch 40 | Loss: 0.0714 | Train Acc: 0.9814\n",
      "Epoch 45 | Loss: 0.0731 | Train Acc: 0.9757\n",
      "Epoch 50 | Loss: 0.0796 | Train Acc: 0.9757\n",
      "Epoch 55 | Loss: 0.0728 | Train Acc: 0.9814\n",
      "Epoch 60 | Loss: 0.0557 | Train Acc: 0.9857\n",
      "Epoch 65 | Loss: 0.0521 | Train Acc: 0.9814\n",
      "Epoch 70 | Loss: 0.0443 | Train Acc: 0.9914\n",
      "Epoch 75 | Loss: 0.0567 | Train Acc: 0.9800\n",
      "Epoch 80 | Loss: 0.0530 | Train Acc: 0.9814\n",
      "\n",
      "Final Test Accuracy: 0.6349\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {total_loss/len(train_loader):.4f} | Train Acc: {correct/total:.4f}\")\n",
    "\n",
    "# Final Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(X_test))\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = (preds == torch.tensor(y_test)).float().mean()\n",
    "    print(f\"\\nFinal Test Accuracy: {acc.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

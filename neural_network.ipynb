{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0992197",
   "metadata": {},
   "source": [
    "# Clean Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538377",
   "metadata": {},
   "source": [
    "## Import and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98909075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Neural Network Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 80\n",
    "VOCAB_SIZE = 500  # Prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = 'cleaned_data'\n",
    "train_file = 'train_clean.csv'\n",
    "valid_file = 'validation_clean.csv'\n",
    "test_file = 'test_clean.csv'\n",
    "path_to_train = os.path.join(curr_dir, data_dir, train_file)\n",
    "path_to_valid = os.path.join(curr_dir, data_dir, valid_file)\n",
    "path_to_test = os.path.join(curr_dir, data_dir, test_file)\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "valid_df = pd.read_csv(path_to_valid)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "valid_df = pd.DataFrame(valid_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1c950",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# suboptimal_example is too noisy will thus be dropped\n",
    "text_cols = [\"tasks_use_model\", \"verify_method\"]\n",
    "numeric_cols = [\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                \"reference_expectation\", \"verify_frequency\"]\n",
    "binary_cols = [c for c in train_df.columns if \"task_types\" in c]\n",
    "\n",
    "best_task_cols = [c for c in train_df.columns if \"best_task_types\" in c]\n",
    "suboptimal_task_cols = [c for c in train_df.columns if \"suboptimal_task_types\" in c]\n",
    "\n",
    "def add_task_sum(df):\n",
    "    df['best_task_count'] = df[best_task_cols].sum(axis=1)\n",
    "    df['suboptimal_task_count'] = df[suboptimal_task_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = add_task_sum(train_df)\n",
    "valid_df = add_task_sum(valid_df)\n",
    "test_df = add_task_sum(test_df)\n",
    "\n",
    "def clean_text(s):\n",
    "    # Convert input to string, handling NaNs, floats, etc.\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s)  \n",
    "    \n",
    "    # Your original cleaning logic\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in text_cols:\n",
    "    # Use .astype(str) on the column before applying for extra safety, \n",
    "    # or just use the improved clean_text function.\n",
    "    train_df[col] = train_df[col].apply(clean_text) \n",
    "    valid_df[col] = valid_df[col].apply(clean_text)\n",
    "    test_df[col] = test_df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df47c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = combine_text(train_df)\n",
    "valid_df = combine_text(valid_df)\n",
    "test_df = combine_text(test_df)\n",
    "\n",
    "# combined_text = train_df[\"full_text\"] + test_df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary (Top 300 words only)\n",
    "word_counts = Counter(train_df[\"full_text\"].str.cat(sep=\" \").split())\n",
    "vocab_list = sorted([w for w, c in word_counts.most_common(VOCAB_SIZE)])\n",
    "vocab_map = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_features_labels(df, text_series, vocab_map, is_train=True):\n",
    "    \n",
    "#     # Extract original numeric cols\n",
    "#     orig_nums = df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "#                     \"reference_expectation\", \"verify_frequency\"]].values\n",
    "#     scaled_orig = (orig_nums - 3.0) / 1.2\n",
    "    \n",
    "#     # Scale the new feature separately\n",
    "#     task_count = df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "#     scaled_count = (task_count - 2.0) / 1.5 \n",
    "\n",
    "#     X_num = np.hstack([scaled_orig, scaled_count])\n",
    "    \n",
    "#     # 2. Binary Features\n",
    "#     X_bin = df[binary_cols].values\n",
    "    \n",
    "#     # 3. Bag of Words (Log Scaled)\n",
    "#     X_bow = np.zeros((len(df), len(vocab_map)), dtype=np.float32)\n",
    "#     for i, text in enumerate(text_series):\n",
    "#         words = text.split()\n",
    "#         for w in words:\n",
    "#             if w in vocab_map:\n",
    "#                 X_bow[i, vocab_map[w]] += 1\n",
    "#     X_bow = np.log1p(X_bow)\n",
    "    \n",
    "#     # Combine\n",
    "#     X = np.hstack([X_num, X_bin, X_bow]).astype(np.float32)\n",
    "    \n",
    "#     if is_train or 'label' in df.columns:\n",
    "#         y = df['label'].values.astype(np.int64)\n",
    "#         return X, y\n",
    "#     return X, None\n",
    "\n",
    "# X_train, y_train = get_features_labels(train_df, train_df[\"full_text\"], vocab_map)\n",
    "# X_valid, y_valid = get_features_labels(valid_df, valid_df[\"full_text\"], vocab_map)\n",
    "# X_test, y_test = get_features_labels(test_df, test_df[\"full_text\"], vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Fit Scalers on TRAIN data only\n",
    "scaler_num = StandardScaler()\n",
    "scaler_count = StandardScaler()\n",
    "\n",
    "# Extract raw data first to fit scalers\n",
    "train_orig_nums = train_df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                            \"reference_expectation\", \"verify_frequency\"]].values\n",
    "train_task_count = train_df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "\n",
    "scaler_num.fit(train_orig_nums)\n",
    "scaler_count.fit(train_task_count)\n",
    "\n",
    "def get_features_labels_robust(df, text_series, vocab_map, scaler_num, scaler_count):\n",
    "    # 1. Numeric Features (Use the fitted scalers)\n",
    "    orig_nums = df[[\"academic_use_likelihood\", \"suboptimal_frequency\", \n",
    "                    \"reference_expectation\", \"verify_frequency\"]].values\n",
    "    # Fill NaNs with 0 or mean before scaling to prevent errors\n",
    "    orig_nums = np.nan_to_num(orig_nums) \n",
    "    scaled_orig = scaler_num.transform(orig_nums)\n",
    "    \n",
    "    task_count = df[[\"best_task_count\", \"suboptimal_task_count\"]].values\n",
    "    task_count = np.nan_to_num(task_count)\n",
    "    scaled_count = scaler_count.transform(task_count)\n",
    "\n",
    "    X_num = np.hstack([scaled_orig, scaled_count])\n",
    "    \n",
    "    # 2. Binary Features (Ensure columns exist, fill missing with 0)\n",
    "    # Uses the global 'binary_cols' list from the training setup\n",
    "    try:\n",
    "        X_bin = df[binary_cols].fillna(0).values\n",
    "    except KeyError:\n",
    "        # Create missing columns if they don't exist in test\n",
    "        for c in binary_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = 0\n",
    "        X_bin = df[binary_cols].values\n",
    "    \n",
    "    # 3. Bag of Words (Same as before)\n",
    "    X_bow = np.zeros((len(df), len(vocab_map)), dtype=np.float32)\n",
    "    for i, text in enumerate(text_series):\n",
    "        words = str(text).split() # Ensure string\n",
    "        for w in words:\n",
    "            if w in vocab_map:\n",
    "                X_bow[i, vocab_map[w]] += 1\n",
    "    X_bow = np.log1p(X_bow)\n",
    "    \n",
    "    X = np.hstack([X_num, X_bin, X_bow]).astype(np.float32)\n",
    "    \n",
    "    y = None\n",
    "    if 'label' in df.columns:\n",
    "        y = df['label'].values.astype(np.int64)\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Re-generate datasets using the robust function\n",
    "X_train, y_train = get_features_labels_robust(train_df, train_df[\"full_text\"], vocab_map, scaler_num, scaler_count)\n",
    "X_valid, y_valid = get_features_labels_robust(valid_df, valid_df[\"full_text\"], vocab_map, scaler_num, scaler_count)\n",
    "X_test, y_test = get_features_labels_robust(test_df, test_df[\"full_text\"], vocab_map, scaler_num, scaler_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc98c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_tensor = torch.utils.data.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "valid_loader = torch.utils.data.DataLoader(valid_tensor, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, 3)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.4) # High dropout for regularization\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# model = MLP(input_dim=X_train.shape[1])\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48484b4c",
   "metadata": {},
   "source": [
    "## Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8e742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[128, 64], dropout_rate=0.4, output_dim=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_dim = h_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c3432ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(params, train_loader, valid_loader, input_dim, patience=8):\n",
    "    model = MLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=params['hidden_layers'],\n",
    "        dropout_rate=params['dropout']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for X_b, y_b in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_b)\n",
    "            loss = criterion(outputs, y_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_v, y_v in valid_loader:\n",
    "                outputs = model(X_v)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == y_v).sum().item()\n",
    "                total += y_v.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Early Stopping Logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0 \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "            \n",
    "    return best_val_acc, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9736421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 48 configurations...\n",
      "LR       | Layers          | Drop  | WD       | Val Acc \n",
      "------------------------------------------------------------\n",
      "0.001    | [128, 64]       | 0.3   | 0.001    | 0.7317\n",
      "0.001    | [128, 64]       | 0.3   | 0.0001   | 0.7317\n",
      "0.001    | [128, 64]       | 0.5   | 0.001    | 0.7236\n",
      "0.001    | [128, 64]       | 0.5   | 0.0001   | 0.7236\n",
      "0.001    | [256, 128]      | 0.3   | 0.001    | 0.7154\n",
      "0.001    | [256, 128]      | 0.3   | 0.0001   | 0.7317\n",
      "0.001    | [256, 128]      | 0.5   | 0.001    | 0.7236\n",
      "0.001    | [256, 128]      | 0.5   | 0.0001   | 0.7317\n",
      "0.001    | [128, 64, 32]   | 0.3   | 0.001    | 0.7154\n",
      "0.001    | [128, 64, 32]   | 0.3   | 0.0001   | 0.6992\n",
      "0.001    | [128, 64, 32]   | 0.5   | 0.001    | 0.7480\n",
      "0.001    | [128, 64, 32]   | 0.5   | 0.0001   | 0.7317\n",
      "0.001    | [64]            | 0.3   | 0.001    | 0.7561\n",
      "0.001    | [64]            | 0.3   | 0.0001   | 0.7561\n",
      "0.001    | [64]            | 0.5   | 0.001    | 0.7480\n",
      "0.001    | [64]            | 0.5   | 0.0001   | 0.7480\n",
      "0.0005   | [128, 64]       | 0.3   | 0.001    | 0.7561\n",
      "0.0005   | [128, 64]       | 0.3   | 0.0001   | 0.7317\n",
      "0.0005   | [128, 64]       | 0.5   | 0.001    | 0.7480\n",
      "0.0005   | [128, 64]       | 0.5   | 0.0001   | 0.7480\n",
      "0.0005   | [256, 128]      | 0.3   | 0.001    | 0.7561\n",
      "0.0005   | [256, 128]      | 0.3   | 0.0001   | 0.7236\n",
      "0.0005   | [256, 128]      | 0.5   | 0.001    | 0.7561\n",
      "0.0005   | [256, 128]      | 0.5   | 0.0001   | 0.7154\n",
      "0.0005   | [128, 64, 32]   | 0.3   | 0.001    | 0.7398\n",
      "0.0005   | [128, 64, 32]   | 0.3   | 0.0001   | 0.7154\n",
      "0.0005   | [128, 64, 32]   | 0.5   | 0.001    | 0.6992\n",
      "0.0005   | [128, 64, 32]   | 0.5   | 0.0001   | 0.7398\n",
      "0.0005   | [64]            | 0.3   | 0.001    | 0.7724\n",
      "0.0005   | [64]            | 0.3   | 0.0001   | 0.7480\n",
      "0.0005   | [64]            | 0.5   | 0.001    | 0.7642\n",
      "0.0005   | [64]            | 0.5   | 0.0001   | 0.7236\n",
      "0.0001   | [128, 64]       | 0.3   | 0.001    | 0.7154\n",
      "0.0001   | [128, 64]       | 0.3   | 0.0001   | 0.7073\n",
      "0.0001   | [128, 64]       | 0.5   | 0.001    | 0.6911\n",
      "0.0001   | [128, 64]       | 0.5   | 0.0001   | 0.6016\n",
      "0.0001   | [256, 128]      | 0.3   | 0.001    | 0.7398\n",
      "0.0001   | [256, 128]      | 0.3   | 0.0001   | 0.7480\n",
      "0.0001   | [256, 128]      | 0.5   | 0.001    | 0.7073\n",
      "0.0001   | [256, 128]      | 0.5   | 0.0001   | 0.7480\n",
      "0.0001   | [128, 64, 32]   | 0.3   | 0.001    | 0.5528\n",
      "0.0001   | [128, 64, 32]   | 0.3   | 0.0001   | 0.5854\n",
      "0.0001   | [128, 64, 32]   | 0.5   | 0.001    | 0.7561\n",
      "0.0001   | [128, 64, 32]   | 0.5   | 0.0001   | 0.3333\n",
      "0.0001   | [64]            | 0.3   | 0.001    | 0.7398\n",
      "0.0001   | [64]            | 0.3   | 0.0001   | 0.7642\n",
      "0.0001   | [64]            | 0.5   | 0.001    | 0.6341\n",
      "0.0001   | [64]            | 0.5   | 0.0001   | 0.7154\n",
      "------------------------------------------------------------\n",
      "Best Accuracy: 0.7724\n",
      "Best Config: {'lr': 0.0005, 'hidden_layers': [64], 'dropout': 0.3, 'weight_decay': 0.001, 'epochs': 80}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter Search Space\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'hidden_layers': [\n",
    "        [128, 64],       # Original structure\n",
    "        [256, 128],      # Wider\n",
    "        [128, 64, 32],   # Deeper\n",
    "        [64]             # Simpler\n",
    "    ],\n",
    "    'dropout': [0.3, 0.5],\n",
    "    'weight_decay': [1e-3, 1e-4],\n",
    "    'epochs': [80] # High cap, controlled by early stopping\n",
    "}\n",
    "\n",
    "best_overall_acc = 0.0\n",
    "best_config = {}\n",
    "best_weights = None\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Generate combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"Testing {len(combinations)} configurations...\")\n",
    "print(f\"{'LR':<8} | {'Layers':<15} | {'Drop':<5} | {'WD':<8} | {'Val Acc':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for params in combinations:\n",
    "    val_acc, weights = grid_search(params, train_loader, valid_loader, input_dim)\n",
    "    \n",
    "    layer_str = str(params['hidden_layers'])\n",
    "    print(f\"{params['lr']:<8} | {layer_str:<15} | {params['dropout']:<5} | {params['weight_decay']:<8} | {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_overall_acc:\n",
    "        best_overall_acc = val_acc\n",
    "        best_config = params\n",
    "        best_weights = weights\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Best Accuracy: {best_overall_acc:.4f}\")\n",
    "print(f\"Best Config: {best_config}\")\n",
    "\n",
    "# Load best weights into a model instance for final testing\n",
    "model = MLP(input_dim, best_config['hidden_layers'], best_config['dropout'])\n",
    "model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2034824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7063491940498352\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Ensure X_test is a tensor\n",
    "    test_tensor = torch.tensor(X_test) if not isinstance(X_test, torch.Tensor) else X_test\n",
    "    logits = model(test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    # Calculate final test accuracy\n",
    "    final_acc = (preds == torch.tensor(y_test)).float().mean().item()\n",
    "    print(f\"Test Accuracy: {final_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f874",
   "metadata": {},
   "source": [
    "## Find Best Validation Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5fc5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0.0\n",
    "# best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# print(f\"{'Epoch':<6} | {'Train Acc':<10} | {'Val Acc':<10}\")\n",
    "# print(\"-\" * 30)\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     # Train\n",
    "#     model.train()\n",
    "#     correct, total = 0, 0\n",
    "#     for X_b, y_b in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_b)\n",
    "#         loss = criterion(outputs, y_b)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         preds = torch.argmax(outputs, dim=1)\n",
    "#         correct += (preds == y_b).sum().item()\n",
    "#         total += y_b.size(0)\n",
    "#     train_acc = correct / total\n",
    "    \n",
    "#     # Validate\n",
    "#     model.eval()\n",
    "#     v_correct, v_total = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for X_v, y_v in valid_loader:\n",
    "#             outputs = model(X_v)\n",
    "#             preds = torch.argmax(outputs, dim=1)\n",
    "#             v_correct += (preds == y_v).sum().item()\n",
    "#             v_total += y_v.size(0)\n",
    "#     val_acc = v_correct / v_total\n",
    "    \n",
    "#     # Checkpoint\n",
    "#     if val_acc > best_acc:\n",
    "#         best_acc = val_acc\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#         print(f\"{epoch+1:<6} | {train_acc:<10.4f} | {val_acc:<10.4f} *\")\n",
    "#     elif (epoch+1) % 10 == 0:\n",
    "#         print(f\"{epoch+1:<6} | {train_acc:<10.4f} | {val_acc:<10.4f}\")\n",
    "\n",
    "# model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Ensure X_test is a tensor\n",
    "#     test_tensor = torch.tensor(X_test) if not isinstance(X_test, torch.Tensor) else X_test\n",
    "#     logits = model(test_tensor)\n",
    "#     preds = torch.argmax(logits, dim=1)\n",
    "#     # Calculate final test accuracy\n",
    "#     final_acc = (preds == torch.tensor(y_test)).float().mean().item()\n",
    "#     print(f\"Test Accuracy: {final_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655d72a",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf66543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = MLP(input_dim=X_train.shape[1])\n",
    "# # model.load_state_dict(best_wts)\n",
    "\n",
    "# # Extract weights (Transposed for x @ W + b shape)\n",
    "# # We convert numpy arrays to lists so they can be serialized to JSON\n",
    "# artifacts = {\n",
    "#     \"W1\": model.fc1.weight.detach().numpy().T.tolist(),\n",
    "#     \"b1\": model.fc1.bias.detach().numpy().tolist(),\n",
    "#     \"W2\": model.fc2.weight.detach().numpy().T.tolist(),\n",
    "#     \"b2\": model.fc2.bias.detach().numpy().tolist(),\n",
    "#     \"W3\": model.fc3.weight.detach().numpy().T.tolist(),\n",
    "#     \"b3\": model.fc3.bias.detach().numpy().tolist(),\n",
    "#     \"vocab_list\": vocab_list,\n",
    "#     \"binary_cols\": binary_cols,\n",
    "#     \"best_task_cols\": best_task_cols,\n",
    "#     \"suboptimal_task_cols\": suboptimal_task_cols\n",
    "# }\n",
    "\n",
    "# # Save to JSON file\n",
    "# with open('model_artifacts.json', 'w') as f:\n",
    "#     json.dump(artifacts, f)\n",
    "\n",
    "# print(\"Successfully saved 'model_artifacts.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb84b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete: model_artifacts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. Extract Model Weights & Biases\n",
    "# We transpose weights (.T) so the math becomes (input @ weights + bias)\n",
    "# This is standard for NumPy inference.\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "for key, param in model.state_dict().items():\n",
    "    if 'weight' in key:\n",
    "        weights.append(param.cpu().detach().numpy().T.tolist())\n",
    "    elif 'bias' in key:\n",
    "        biases.append(param.cpu().detach().numpy().tolist())\n",
    "\n",
    "# 2. Extract Scaler Statistics (Critical for correct input scaling)\n",
    "# Check if scalers are fitted; if using the manual method, you might need to hardcode these\n",
    "scaler_data = {\n",
    "    \"num_mean\": scaler_num.mean_.tolist(),\n",
    "    \"num_scale\": scaler_num.scale_.tolist(),\n",
    "    \"count_mean\": scaler_count.mean_.tolist(),\n",
    "    \"count_scale\": scaler_count.scale_.tolist()\n",
    "}\n",
    "\n",
    "# 3. Bundle Everything\n",
    "artifacts = {\n",
    "    \"weights\": weights,\n",
    "    \"biases\": biases,\n",
    "    \"vocab_map\": vocab_map,\n",
    "    \"binary_cols\": binary_cols, # List of binary column names\n",
    "    \"scalers\": scaler_data\n",
    "}\n",
    "\n",
    "with open('model_artifacts.json', 'w') as f:\n",
    "    json.dump(artifacts, f)\n",
    "\n",
    "print(\"Export complete: model_artifacts.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0992197",
   "metadata": {},
   "source": [
    "# Clean Dataset and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08538377",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98909075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e177a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_dir = 'cleaned_data'\n",
    "train_file = 'train_clean.csv'\n",
    "valid_file = 'validation_clean.csv'\n",
    "test_file = 'test_clean.csv'\n",
    "path_to_train = os.path.join(curr_dir, data_dir, train_file)\n",
    "path_to_valid = os.path.join(curr_dir, data_dir, valid_file)\n",
    "path_to_test = os.path.join(curr_dir, data_dir, test_file)\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "valid_df = pd.read_csv(path_to_valid)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "valid_df = pd.DataFrame(valid_df)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3179d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "d395eaf2-5acb-4351-8eda-9793ca225d15",
       "rows": [
        [
         "student_id",
         "int64"
        ],
        [
         "tasks_use_model",
         "object"
        ],
        [
         "academic_use_likelihood",
         "int64"
        ],
        [
         "suboptimal_frequency",
         "float64"
        ],
        [
         "suboptimal_example",
         "object"
        ],
        [
         "reference_expectation",
         "float64"
        ],
        [
         "verify_frequency",
         "float64"
        ],
        [
         "verify_method",
         "object"
        ],
        [
         "label",
         "int64"
        ],
        [
         "best_task_types_brainstorming_or_generating_creative_ideas",
         "int64"
        ],
        [
         "best_task_types_converting_content_between_formats",
         "int64"
        ],
        [
         "best_task_types_data_processing_or_analysis",
         "int64"
        ],
        [
         "best_task_types_drafting_professional_text",
         "int64"
        ],
        [
         "best_task_types_explaining_complex_concepts_simply",
         "int64"
        ],
        [
         "best_task_types_math_computations",
         "int64"
        ],
        [
         "best_task_types_writing_or_debugging_code",
         "int64"
        ],
        [
         "best_task_types_writing_or_editing_essays_reports",
         "int64"
        ],
        [
         "suboptimal_task_types_brainstorming_or_generating_creative_ideas",
         "int64"
        ],
        [
         "suboptimal_task_types_converting_content_between_formats",
         "int64"
        ],
        [
         "suboptimal_task_types_data_processing_or_analysis",
         "int64"
        ],
        [
         "suboptimal_task_types_drafting_professional_text",
         "int64"
        ],
        [
         "suboptimal_task_types_explaining_complex_concepts_simply",
         "int64"
        ],
        [
         "suboptimal_task_types_math_computations",
         "int64"
        ],
        [
         "suboptimal_task_types_writing_or_debugging_code",
         "int64"
        ],
        [
         "suboptimal_task_types_writing_or_editing_essays_reports",
         "int64"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 25
       }
      },
      "text/plain": [
       "student_id                                                            int64\n",
       "tasks_use_model                                                      object\n",
       "academic_use_likelihood                                               int64\n",
       "suboptimal_frequency                                                float64\n",
       "suboptimal_example                                                   object\n",
       "reference_expectation                                               float64\n",
       "verify_frequency                                                    float64\n",
       "verify_method                                                        object\n",
       "label                                                                 int64\n",
       "best_task_types_brainstorming_or_generating_creative_ideas            int64\n",
       "best_task_types_converting_content_between_formats                    int64\n",
       "best_task_types_data_processing_or_analysis                           int64\n",
       "best_task_types_drafting_professional_text                            int64\n",
       "best_task_types_explaining_complex_concepts_simply                    int64\n",
       "best_task_types_math_computations                                     int64\n",
       "best_task_types_writing_or_debugging_code                             int64\n",
       "best_task_types_writing_or_editing_essays_reports                     int64\n",
       "suboptimal_task_types_brainstorming_or_generating_creative_ideas      int64\n",
       "suboptimal_task_types_converting_content_between_formats              int64\n",
       "suboptimal_task_types_data_processing_or_analysis                     int64\n",
       "suboptimal_task_types_drafting_professional_text                      int64\n",
       "suboptimal_task_types_explaining_complex_concepts_simply              int64\n",
       "suboptimal_task_types_math_computations                               int64\n",
       "suboptimal_task_types_writing_or_debugging_code                       int64\n",
       "suboptimal_task_types_writing_or_editing_essays_reports               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_cols = [\"tasks_use_model\", \"suboptimal_example\", \"verify_method\"]\n",
    "\n",
    "def clean_text(s):\n",
    "    # Convert input to string, handling NaNs, floats, etc.\n",
    "    if s is None:\n",
    "        s = \"\"\n",
    "    s = str(s)  \n",
    "    \n",
    "    # Your original cleaning logic\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in text_cols:\n",
    "    # Use .astype(str) on the column before applying for extra safety, \n",
    "    # or just use the improved clean_text function.\n",
    "    train_df[col] = train_df[col].apply(clean_text) \n",
    "    valid_df[col] = valid_df[col].apply(clean_text)\n",
    "    test_df[col] = test_df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df47c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = [\"tasks_use_model\", \"suboptimal_example\", \"verify_method\"]\n",
    "\n",
    "def combine_text(df):\n",
    "    df[\"full_text\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = combine_text(train_df)\n",
    "valid_df = combine_text(valid_df)\n",
    "test_df = combine_text(test_df)\n",
    "\n",
    "combined_text = train_df[\"full_text\"] + valid_df[\"full_text\"] + test_df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = combined_text.str.cat(sep=' ')\n",
    "\n",
    "all_words = full_corpus.split()\n",
    "\n",
    "vocab_vector = np.unique(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458d5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training BoW Matrix: (576, 2738)\n",
      "Example of first row (document counts): [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_text_to_bow(text_series, vocab_vector):\n",
    "    \"\"\"\n",
    "    Converts a pandas Series of text into a raw Bag-of-Words count NumPy array\n",
    "    based on a provided vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create a dictionary map for fast vocabulary lookup\n",
    "    # This maps the word to its column index in the final matrix\n",
    "    vocab_map = {word: i for i, word in enumerate(vocab_vector)}\n",
    "    vocab_size = len(vocab_vector)\n",
    "    num_documents = len(text_series)\n",
    "    \n",
    "    # Initialize the count matrix (BoW)\n",
    "    # Using integer type for simple counts\n",
    "    X_bow = np.zeros((num_documents, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    # 2. Fill the BoW count matrix\n",
    "    for doc_index, document in enumerate(text_series):\n",
    "        # The text is assumed to be cleaned and lowercased already\n",
    "        words = document.split()\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocab_map:\n",
    "                word_index = vocab_map[word]\n",
    "                # Increment the count for this word in this document\n",
    "                X_bow[doc_index, word_index] += 1\n",
    "                \n",
    "    return X_bow\n",
    "\n",
    "X_train_bow = encode_text_to_bow(train_df['full_text'], vocab_vector)\n",
    "X_valid_bow = encode_text_to_bow(valid_df['full_text'], vocab_vector)\n",
    "X_test_bow = encode_text_to_bow(test_df['full_text'], vocab_vector)\n",
    "\n",
    "print(f\"Shape of Training BoW Matrix: {X_train_bow.shape}\")\n",
    "print(f\"Example of first row (document counts): {X_train_bow[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0377fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = pd.DataFrame([train_df['academic_use_likelihood'], \n",
    "                        train_df['suboptimal_frequency'], \n",
    "                        train_df['reference_expectation'],\n",
    "                        train_df['verify_frequency']]).transpose()\n",
    "\n",
    "valid_encoded = pd.DataFrame([valid_df['academic_use_likelihood'], \n",
    "                        valid_df['suboptimal_frequency'], \n",
    "                        valid_df['reference_expectation'],\n",
    "                        valid_df['verify_frequency']]).transpose()\n",
    "\n",
    "test_encoded = pd.DataFrame([test_df['academic_use_likelihood'], \n",
    "                        test_df['suboptimal_frequency'], \n",
    "                        test_df['reference_expectation'],\n",
    "                        test_df['verify_frequency']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8cef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = pd.concat([train_encoded, pd.DataFrame(X_train_bow)],ignore_index=True, sort=False, axis=1)\n",
    "valid_encoded = pd.concat([valid_encoded, pd.DataFrame(X_valid_bow)],ignore_index=True, sort=False, axis=1)\n",
    "test_encoded = pd.concat([test_encoded, pd.DataFrame(X_test_bow)],ignore_index=True, sort=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a3eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = np.stack([train_df['label']], axis=1).reshape(-1)\n",
    "valid_t = np.stack([valid_df['label']], axis=1).reshape(-1)\n",
    "test_t = np.stack([test_df['label']], axis=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09615bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a58b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd533655",
   "metadata": {},
   "source": [
    "SKIP THESE CODE BLOCKS. KEEPING IT STILL JUST FOR REFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99a842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (576, 1620)\n",
      "y shape: (576,)\n",
      "num_classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X_num_cat = train_df[num_cols + cat_cols].copy()\n",
    "y = train_df[target_col].values\n",
    "\n",
    "# Scale numeric columns (on full dataset)\n",
    "scaler = StandardScaler()\n",
    "if num_cols:\n",
    "    X_num_scaled = scaler.fit_transform(X_num_cat[num_cols])\n",
    "    X_num_cat[num_cols] = X_num_scaled\n",
    "\n",
    "X_num_cat = X_num_cat.to_numpy().astype(np.float32)  # numeric + cat\n",
    "X_bow = bow_matrix.astype(np.float32)\n",
    "\n",
    "# Final feature matrix: concat [numeric+cat, BoW]\n",
    "X = np.concatenate([X_num_cat, X_bow], axis=1)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "N, input_dim = X.shape\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_num_cat = valid_df[num_cols + cat_cols].copy()\n",
    "\n",
    "# Scale numeric columns with the same scaler\n",
    "if num_cols:\n",
    "    X_val_num = scaler.transform(X_val_num_cat[num_cols])\n",
    "    X_val_num_cat[num_cols] = X_val_num\n",
    "\n",
    "X_val_num_cat = X_val_num_cat.to_numpy().astype(np.float32)\n",
    "X_val = np.concatenate([X_val_num_cat, bow_matrix_val], axis=1)\n",
    "\n",
    "y_val = valid_df[target_col].astype(np.int64).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cfc69",
   "metadata": {},
   "source": [
    "END SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480da51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5cdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be9c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    torch.from_numpy(train_encoded.to_numpy().astype(np.float32)),\n",
    "    torch.from_numpy(train_t.astype(np.int64))\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02589c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "val_ds = TensorDataset(\n",
    "    torch.from_numpy(valid_encoded.to_numpy().astype(np.float32)),\n",
    "    torch.from_numpy(valid_t.astype(np.int64))\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b08ba207",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_ds = TensorDataset(\n",
    "    torch.from_numpy(test_encoded.to_numpy().astype(np.float32)),\n",
    "    torch.from_numpy(test_t.astype(np.int64))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c2663f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=train_encoded.to_numpy().shape[1]\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17189e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2742])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e9e3865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPBoW(\n",
       "  (fc1): Linear(in_features=2742, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLPBoW(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_classes=3, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPBoW(input_dim=input_dim, hidden_dim=64, num_classes=num_classes, dropout_p=0.3).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20c18958",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2af2e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch   5 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  10 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  15 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  20 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  25 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  30 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  35 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  40 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  45 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  50 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  55 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  60 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  65 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  70 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  75 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  80 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  85 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  90 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch  95 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n",
      "Epoch 100 | train_loss=1.0984 | train_acc=0.3524 | val_loss=1.0993 | val_acc=0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "min_delta = 0.0\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # === After epoch: evaluate ===\n",
    "    train_loss, train_acc = evaluate(train_loader)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d} | \"\n",
    "            f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "# restore best model\n",
    "model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc8a5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.293651\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1) if logits.dim() > 1 else (logits.view(-1) > 0.5).long()\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "test_acc = float(correct) / float(total) if total else 0.0\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
